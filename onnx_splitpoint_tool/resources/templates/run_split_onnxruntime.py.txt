#!/usr/bin/env python3
# Auto-generated by onnx_splitpoint_tool
# Runner: ORT benchmark for full / part1 / part2 / composed (+ optional CV viz)
#
# Features:
# - provider selection: CPU / CUDA / TensorRT (with cache + fast-build preset)
# - inputs: random generation, optional --inputs-npz, optional image auto-feed (test_image.png)
# - output diff (max_abs/mean_abs) with --eps
# - optional report plots (validation_report.png/.pdf) if matplotlib is available
# - optional YOLO visualization (detections_*.png/.json) if PIL is available

from __future__ import annotations

import argparse
import ast
import json
import os
import sys
import threading
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple

import numpy as np
import onnx
import onnxruntime as ort

# Optional deps (runner must still work without them)
try:
    from PIL import Image, ImageDraw  # type: ignore
except Exception:
    Image = None  # type: ignore
    ImageDraw = None  # type: ignore


# ImageNet-1k labels (0..999)
IMAGENET1K_LABELS: List[str] = [
    "tench",
    "goldfish",
    "great white shark",
    "tiger shark",
    "hammerhead",
    "electric ray",
    "stingray",
    "cock",
    "hen",
    "ostrich",
    "brambling",
    "goldfinch",
    "house finch",
    "junco",
    "indigo bunting",
    "robin",
    "bulbul",
    "jay",
    "magpie",
    "chickadee",
    "water ouzel",
    "kite",
    "bald eagle",
    "vulture",
    "great grey owl",
    "European fire salamander",
    "common newt",
    "eft",
    "spotted salamander",
    "axolotl",
    "bullfrog",
    "tree frog",
    "tailed frog",
    "loggerhead",
    "leatherback turtle",
    "mud turtle",
    "terrapin",
    "box turtle",
    "banded gecko",
    "common iguana",
    "American chameleon",
    "whiptail",
    "agama",
    "frilled lizard",
    "alligator lizard",
    "Gila monster",
    "green lizard",
    "African chameleon",
    "Komodo dragon",
    "African crocodile",
    "American alligator",
    "triceratops",
    "thunder snake",
    "ringneck snake",
    "hognose snake",
    "green snake",
    "king snake",
    "garter snake",
    "water snake",
    "vine snake",
    "night snake",
    "boa constrictor",
    "rock python",
    "Indian cobra",
    "green mamba",
    "sea snake",
    "horned viper",
    "diamondback",
    "sidewinder",
    "trilobite",
    "harvestman",
    "scorpion",
    "black and gold garden spider",
    "barn spider",
    "garden spider",
    "black widow",
    "tarantula",
    "wolf spider",
    "tick",
    "centipede",
    "black grouse",
    "ptarmigan",
    "ruffed grouse",
    "prairie chicken",
    "peacock",
    "quail",
    "partridge",
    "African grey",
    "macaw",
    "sulphur-crested cockatoo",
    "lorikeet",
    "coucal",
    "bee eater",
    "hornbill",
    "hummingbird",
    "jacamar",
    "toucan",
    "drake",
    "red-breasted merganser",
    "goose",
    "black swan",
    "tusker",
    "echidna",
    "platypus",
    "wallaby",
    "koala",
    "wombat",
    "jellyfish",
    "sea anemone",
    "brain coral",
    "flatworm",
    "nematode",
    "conch",
    "snail",
    "slug",
    "sea slug",
    "chiton",
    "chambered nautilus",
    "Dungeness crab",
    "rock crab",
    "fiddler crab",
    "king crab",
    "American lobster",
    "spiny lobster",
    "crayfish",
    "hermit crab",
    "isopod",
    "white stork",
    "black stork",
    "spoonbill",
    "flamingo",
    "little blue heron",
    "American egret",
    "bittern",
    "crane bird",
    "limpkin",
    "European gallinule",
    "American coot",
    "bustard",
    "ruddy turnstone",
    "red-backed sandpiper",
    "redshank",
    "dowitcher",
    "oystercatcher",
    "pelican",
    "king penguin",
    "albatross",
    "grey whale",
    "killer whale",
    "dugong",
    "sea lion",
    "Chihuahua",
    "Japanese spaniel",
    "Maltese dog",
    "Pekinese",
    "Shih-Tzu",
    "Blenheim spaniel",
    "papillon",
    "toy terrier",
    "Rhodesian ridgeback",
    "Afghan hound",
    "basset",
    "beagle",
    "bloodhound",
    "bluetick",
    "black-and-tan coonhound",
    "Walker hound",
    "English foxhound",
    "redbone",
    "borzoi",
    "Irish wolfhound",
    "Italian greyhound",
    "whippet",
    "Ibizan hound",
    "Norwegian elkhound",
    "otterhound",
    "Saluki",
    "Scottish deerhound",
    "Weimaraner",
    "Staffordshire bullterrier",
    "American Staffordshire terrier",
    "Bedlington terrier",
    "Border terrier",
    "Kerry blue terrier",
    "Irish terrier",
    "Norfolk terrier",
    "Norwich terrier",
    "Yorkshire terrier",
    "wire-haired fox terrier",
    "Lakeland terrier",
    "Sealyham terrier",
    "Airedale",
    "cairn",
    "Australian terrier",
    "Dandie Dinmont",
    "Boston bull",
    "miniature schnauzer",
    "giant schnauzer",
    "standard schnauzer",
    "Scotch terrier",
    "Tibetan terrier",
    "silky terrier",
    "soft-coated wheaten terrier",
    "West Highland white terrier",
    "Lhasa",
    "flat-coated retriever",
    "curly-coated retriever",
    "golden retriever",
    "Labrador retriever",
    "Chesapeake Bay retriever",
    "German short-haired pointer",
    "vizsla",
    "English setter",
    "Irish setter",
    "Gordon setter",
    "Brittany spaniel",
    "clumber",
    "English springer",
    "Welsh springer spaniel",
    "cocker spaniel",
    "Sussex spaniel",
    "Irish water spaniel",
    "kuvasz",
    "schipperke",
    "groenendael",
    "malinois",
    "briard",
    "kelpie",
    "komondor",
    "Old English sheepdog",
    "Shetland sheepdog",
    "collie",
    "Border collie",
    "Bouvier des Flandres",
    "Rottweiler",
    "German shepherd",
    "Doberman",
    "miniature pinscher",
    "Greater Swiss Mountain dog",
    "Bernese mountain dog",
    "Appenzeller",
    "EntleBucher",
    "boxer",
    "bull mastiff",
    "Tibetan mastiff",
    "French bulldog",
    "Great Dane",
    "Saint Bernard",
    "Eskimo dog",
    "malamute",
    "Siberian husky",
    "dalmatian",
    "affenpinscher",
    "basenji",
    "pug",
    "Leonberg",
    "Newfoundland",
    "Great Pyrenees",
    "Samoyed",
    "Pomeranian",
    "chow",
    "keeshond",
    "Brabancon griffon",
    "Pembroke",
    "Cardigan",
    "toy poodle",
    "miniature poodle",
    "standard poodle",
    "Mexican hairless",
    "timber wolf",
    "white wolf",
    "red wolf",
    "coyote",
    "dingo",
    "dhole",
    "African hunting dog",
    "hyena",
    "red fox",
    "kit fox",
    "Arctic fox",
    "grey fox",
    "tabby",
    "tiger cat",
    "Persian cat",
    "Siamese cat",
    "Egyptian cat",
    "cougar",
    "lynx",
    "leopard",
    "snow leopard",
    "jaguar",
    "lion",
    "tiger",
    "cheetah",
    "brown bear",
    "American black bear",
    "ice bear",
    "sloth bear",
    "mongoose",
    "meerkat",
    "tiger beetle",
    "ladybug",
    "ground beetle",
    "long-horned beetle",
    "leaf beetle",
    "dung beetle",
    "rhinoceros beetle",
    "weevil",
    "fly",
    "bee",
    "ant",
    "grasshopper",
    "cricket",
    "walking stick",
    "cockroach",
    "mantis",
    "cicada",
    "leafhopper",
    "lacewing",
    "dragonfly",
    "damselfly",
    "admiral",
    "ringlet",
    "monarch",
    "cabbage butterfly",
    "sulphur butterfly",
    "lycaenid",
    "starfish",
    "sea urchin",
    "sea cucumber",
    "wood rabbit",
    "hare",
    "Angora",
    "hamster",
    "porcupine",
    "fox squirrel",
    "marmot",
    "beaver",
    "guinea pig",
    "sorrel",
    "zebra",
    "hog",
    "wild boar",
    "warthog",
    "hippopotamus",
    "ox",
    "water buffalo",
    "bison",
    "ram",
    "bighorn",
    "ibex",
    "hartebeest",
    "impala",
    "gazelle",
    "Arabian camel",
    "llama",
    "weasel",
    "mink",
    "polecat",
    "black-footed ferret",
    "otter",
    "skunk",
    "badger",
    "armadillo",
    "three-toed sloth",
    "orangutan",
    "gorilla",
    "chimpanzee",
    "gibbon",
    "siamang",
    "guenon",
    "patas",
    "baboon",
    "macaque",
    "langur",
    "colobus",
    "proboscis monkey",
    "marmoset",
    "capuchin",
    "howler monkey",
    "titi",
    "spider monkey",
    "squirrel monkey",
    "Madagascar cat",
    "indri",
    "Indian elephant",
    "African elephant",
    "lesser panda",
    "giant panda",
    "barracouta",
    "eel",
    "coho",
    "rock beauty",
    "anemone fish",
    "sturgeon",
    "gar",
    "lionfish",
    "puffer",
    "abacus",
    "abaya",
    "academic gown",
    "accordion",
    "acoustic guitar",
    "aircraft carrier",
    "airliner",
    "airship",
    "altar",
    "ambulance",
    "amphibian",
    "analog clock",
    "apiary",
    "apron",
    "ashcan",
    "assault rifle",
    "backpack",
    "bakery",
    "balance beam",
    "balloon",
    "ballpoint",
    "Band Aid",
    "banjo",
    "bannister",
    "barbell",
    "barber chair",
    "barbershop",
    "barn",
    "barometer",
    "barrel",
    "barrow",
    "baseball",
    "basketball",
    "bassinet",
    "bassoon",
    "bathing cap",
    "bath towel",
    "bathtub",
    "beach wagon",
    "beacon",
    "beaker",
    "bearskin",
    "beer bottle",
    "beer glass",
    "bell cote",
    "bib",
    "bicycle-built-for-two",
    "bikini",
    "binder",
    "binoculars",
    "birdhouse",
    "boathouse",
    "bobsled",
    "bolo tie",
    "bonnet",
    "bookcase",
    "bookshop",
    "bottlecap",
    "bow",
    "bow tie",
    "brass",
    "brassiere",
    "breakwater",
    "breastplate",
    "broom",
    "bucket",
    "buckle",
    "bulletproof vest",
    "bullet train",
    "butcher shop",
    "cab",
    "caldron",
    "candle",
    "cannon",
    "canoe",
    "can opener",
    "cardigan",
    "car mirror",
    "carousel",
    "carpenter's kit",
    "carton",
    "car wheel",
    "cash machine",
    "cassette",
    "cassette player",
    "castle",
    "catamaran",
    "CD player",
    "cello",
    "cellular telephone",
    "chain",
    "chainlink fence",
    "chain mail",
    "chain saw",
    "chest",
    "chiffonier",
    "chime",
    "china cabinet",
    "Christmas stocking",
    "church",
    "cinema",
    "cleaver",
    "cliff dwelling",
    "cloak",
    "clog",
    "cocktail shaker",
    "coffee mug",
    "coffeepot",
    "coil",
    "combination lock",
    "computer keyboard",
    "confectionery",
    "container ship",
    "convertible",
    "corkscrew",
    "cornet",
    "cowboy boot",
    "cowboy hat",
    "cradle",
    "crane",
    "crash helmet",
    "crate",
    "crib",
    "Crock Pot",
    "croquet ball",
    "crutch",
    "cuirass",
    "dam",
    "desk",
    "desktop computer",
    "dial telephone",
    "diaper",
    "digital clock",
    "digital watch",
    "dining table",
    "dishrag",
    "dishwasher",
    "disk brake",
    "dock",
    "dogsled",
    "dome",
    "doormat",
    "drilling platform",
    "drum",
    "drumstick",
    "dumbbell",
    "Dutch oven",
    "electric fan",
    "electric guitar",
    "electric locomotive",
    "entertainment center",
    "envelope",
    "espresso maker",
    "face powder",
    "feather boa",
    "file",
    "fireboat",
    "fire engine",
    "fire screen",
    "flagpole",
    "flute",
    "folding chair",
    "football helmet",
    "forklift",
    "fountain",
    "fountain pen",
    "four-poster",
    "freight car",
    "French horn",
    "frying pan",
    "fur coat",
    "garbage truck",
    "gasmask",
    "gas pump",
    "goblet",
    "go-kart",
    "golf ball",
    "golfcart",
    "gondola",
    "gong",
    "gown",
    "grand piano",
    "greenhouse",
    "grille",
    "grocery store",
    "guillotine",
    "hair slide",
    "hair spray",
    "half track",
    "hammer",
    "hamper",
    "hand blower",
    "hand-held computer",
    "handkerchief",
    "hard disc",
    "harmonica",
    "harp",
    "harvester",
    "hatchet",
    "holster",
    "home theater",
    "honeycomb",
    "hook",
    "hoopskirt",
    "horizontal bar",
    "horse cart",
    "hourglass",
    "iPod",
    "iron",
    "jack-o'-lantern",
    "jean",
    "jeep",
    "jersey",
    "jigsaw puzzle",
    "jinrikisha",
    "joystick",
    "kimono",
    "knee pad",
    "knot",
    "lab coat",
    "ladle",
    "lampshade",
    "laptop",
    "lawn mower",
    "lens cap",
    "letter opener",
    "library",
    "lifeboat",
    "lighter",
    "limousine",
    "liner",
    "lipstick",
    "Loafer",
    "lotion",
    "loudspeaker",
    "loupe",
    "lumbermill",
    "magnetic compass",
    "mailbag",
    "mailbox",
    "maillot",
    "maillot tank suit",
    "manhole cover",
    "maraca",
    "marimba",
    "mask",
    "matchstick",
    "maypole",
    "maze",
    "measuring cup",
    "medicine chest",
    "megalith",
    "microphone",
    "microwave",
    "military uniform",
    "milk can",
    "minibus",
    "miniskirt",
    "minivan",
    "missile",
    "mitten",
    "mixing bowl",
    "mobile home",
    "Model T",
    "modem",
    "monastery",
    "monitor",
    "moped",
    "mortar",
    "mortarboard",
    "mosque",
    "mosquito net",
    "motor scooter",
    "mountain bike",
    "mountain tent",
    "mouse",
    "mousetrap",
    "moving van",
    "muzzle",
    "nail",
    "neck brace",
    "necklace",
    "nipple",
    "notebook",
    "obelisk",
    "oboe",
    "ocarina",
    "odometer",
    "oil filter",
    "organ",
    "oscilloscope",
    "overskirt",
    "oxcart",
    "oxygen mask",
    "packet",
    "paddle",
    "paddlewheel",
    "padlock",
    "paintbrush",
    "pajama",
    "palace",
    "panpipe",
    "paper towel",
    "parachute",
    "parallel bars",
    "park bench",
    "parking meter",
    "passenger car",
    "patio",
    "pay-phone",
    "pedestal",
    "pencil box",
    "pencil sharpener",
    "perfume",
    "Petri dish",
    "photocopier",
    "pick",
    "pickelhaube",
    "picket fence",
    "pickup",
    "pier",
    "piggy bank",
    "pill bottle",
    "pillow",
    "ping-pong ball",
    "pinwheel",
    "pirate",
    "pitcher",
    "plane",
    "planetarium",
    "plastic bag",
    "plate rack",
    "plow",
    "plunger",
    "Polaroid camera",
    "pole",
    "police van",
    "poncho",
    "pool table",
    "pop bottle",
    "pot",
    "potter's wheel",
    "power drill",
    "prayer rug",
    "printer",
    "prison",
    "projectile",
    "projector",
    "puck",
    "punching bag",
    "purse",
    "quill",
    "quilt",
    "racer",
    "racket",
    "radiator",
    "radio",
    "radio telescope",
    "rain barrel",
    "recreational vehicle",
    "reel",
    "reflex camera",
    "refrigerator",
    "remote control",
    "restaurant",
    "revolver",
    "rifle",
    "rocking chair",
    "rotisserie",
    "rubber eraser",
    "rugby ball",
    "rule",
    "running shoe",
    "safe",
    "safety pin",
    "saltshaker",
    "sandal",
    "sarong",
    "sax",
    "scabbard",
    "scale",
    "school bus",
    "schooner",
    "scoreboard",
    "screen",
    "screw",
    "screwdriver",
    "seat belt",
    "sewing machine",
    "shield",
    "shoe shop",
    "shoji",
    "shopping basket",
    "shopping cart",
    "shovel",
    "shower cap",
    "shower curtain",
    "ski",
    "ski mask",
    "sleeping bag",
    "slide rule",
    "sliding door",
    "slot",
    "snorkel",
    "snowmobile",
    "snowplow",
    "soap dispenser",
    "soccer ball",
    "sock",
    "solar dish",
    "sombrero",
    "soup bowl",
    "space bar",
    "space heater",
    "space shuttle",
    "spatula",
    "speedboat",
    "spider web",
    "spindle",
    "sports car",
    "spotlight",
    "stage",
    "steam locomotive",
    "steel arch bridge",
    "steel drum",
    "stethoscope",
    "stole",
    "stone wall",
    "stopwatch",
    "stove",
    "strainer",
    "streetcar",
    "stretcher",
    "studio couch",
    "stupa",
    "submarine",
    "suit",
    "sundial",
    "sunglass",
    "sunglasses",
    "sunscreen",
    "suspension bridge",
    "swab",
    "sweatshirt",
    "swimming trunks",
    "swing",
    "switch",
    "syringe",
    "table lamp",
    "tank",
    "tape player",
    "teapot",
    "teddy",
    "television",
    "tennis ball",
    "thatch",
    "theater curtain",
    "thimble",
    "thresher",
    "throne",
    "tile roof",
    "toaster",
    "tobacco shop",
    "toilet seat",
    "torch",
    "totem pole",
    "tow truck",
    "toyshop",
    "tractor",
    "trailer truck",
    "tray",
    "trench coat",
    "tricycle",
    "trimaran",
    "tripod",
    "triumphal arch",
    "trolleybus",
    "trombone",
    "tub",
    "turnstile",
    "typewriter keyboard",
    "umbrella",
    "unicycle",
    "upright",
    "vacuum",
    "vase",
    "vault",
    "velvet",
    "vending machine",
    "vestment",
    "viaduct",
    "violin",
    "volleyball",
    "waffle iron",
    "wall clock",
    "wallet",
    "wardrobe",
    "warplane",
    "washbasin",
    "washer",
    "water bottle",
    "water jug",
    "water tower",
    "whiskey jug",
    "whistle",
    "wig",
    "window screen",
    "window shade",
    "Windsor tie",
    "wine bottle",
    "wing",
    "wok",
    "wooden spoon",
    "wool",
    "worm fence",
    "wreck",
    "yawl",
    "yurt",
    "web site",
    "comic book",
    "crossword puzzle",
    "street sign",
    "traffic light",
    "book jacket",
    "menu",
    "plate",
    "guacamole",
    "consomme",
    "hot pot",
    "trifle",
    "ice cream",
    "ice lolly",
    "French loaf",
    "bagel",
    "pretzel",
    "cheeseburger",
    "hotdog",
    "mashed potato",
    "head cabbage",
    "broccoli",
    "cauliflower",
    "zucchini",
    "spaghetti squash",
    "acorn squash",
    "butternut squash",
    "cucumber",
    "artichoke",
    "bell pepper",
    "cardoon",
    "mushroom",
    "Granny Smith",
    "strawberry",
    "orange",
    "lemon",
    "fig",
    "pineapple",
    "banana",
    "jackfruit",
    "custard apple",
    "pomegranate",
    "hay",
    "carbonara",
    "chocolate sauce",
    "dough",
    "meat loaf",
    "pizza",
    "potpie",
    "burrito",
    "red wine",
    "espresso",
    "cup",
    "eggnog",
    "alp",
    "bubble",
    "cliff",
    "coral reef",
    "geyser",
    "lakeside",
    "promontory",
    "sandbar",
    "seashore",
    "valley",
    "volcano",
    "ballplayer",
    "groom",
    "scuba diver",
    "rapeseed",
    "daisy",
    "yellow lady's slipper",
    "corn",
    "acorn",
    "hip",
    "buckeye",
    "coral fungus",
    "agaric",
    "gyromitra",
    "stinkhorn",
    "earthstar",
    "hen-of-the-woods",
    "bolete",
    "ear",
    "toilet tissue",
]


# ----------------------------
# Optional: SplitPoint runtime
# ----------------------------
# New benchmark suites may ship a small runtime library at suite-root ("splitpoint_runners/")
# which provides GraphRunner + backends. This runner template will opportunistically use it
# when available, and fall back to the legacy in-file benchmarking loop otherwise.


def _maybe_add_suite_runtime_to_syspath() -> None:
    """Add parent dirs containing 'splitpoint_runners' to sys.path (best-effort)."""

    try:
        here = Path(__file__).resolve().parent
    except Exception:
        return

    for cand in (here, here.parent, here.parent.parent):
        try:
            if (cand / "splitpoint_runners").is_dir():
                p = str(cand)
                if p not in sys.path:
                    sys.path.insert(0, p)
                return
        except Exception:
            continue


_maybe_add_suite_runtime_to_syspath()

_HAS_SPLITPOINT_RUNNERS = False
try:
    from splitpoint_runners.graph_runner import GraphRunner as _SP_GraphRunner
    from splitpoint_runners._types import BackendRunOut as _SP_BackendRunOut
    from splitpoint_runners._types import GraphPlan as _SP_GraphPlan
    from splitpoint_runners._types import RunCfg as _SP_RunCfg
    from splitpoint_runners._types import SampleCfg as _SP_SampleCfg
    from splitpoint_runners._types import Stage as _SP_Stage
    from splitpoint_runners.backends.base import BackendCaps as _SP_BackendCaps
    from splitpoint_runners.interface_transfer import DefaultTransfer as _SP_DefaultTransfer
    from splitpoint_runners.interface_transfer import TransferMeta as _SP_TransferMeta

    _HAS_SPLITPOINT_RUNNERS = True
except Exception:
    _HAS_SPLITPOINT_RUNNERS = False


if _HAS_SPLITPOINT_RUNNERS:

    class _SP_StaticHarness:
        """Harness that returns a static pre-built input dictionary."""

        name = "static_inputs"

        def __init__(self, inputs: Dict[str, np.ndarray]):
            self._inputs = inputs

        def make_inputs(self, sample_cfg: _SP_SampleCfg) -> Dict[str, np.ndarray]:  # noqa: ARG002
            return self._inputs

        def postprocess(self, outputs: Dict[str, np.ndarray], context: Dict[str, object]) -> Dict[str, np.ndarray]:  # noqa: ARG002
            return outputs

        def accuracy_proxy(self, ref: Dict[str, np.ndarray], out: Dict[str, np.ndarray]) -> Dict[str, object]:  # noqa: ARG002
            return {}

    class _SP_OrtSessionBackend:
        """Backend wrapper around an already-built onnxruntime.InferenceSession."""

        def __init__(self, name: str, sess: "ort.InferenceSession"):
            self.name = name
            self.capabilities = _SP_BackendCaps(
                supports_fp16=True,
                supports_cache_dir=True,
                needs_compiler=False,
                supports_two_stage=True,
            )
            self._sess = sess
            self._input_names = {i.name for i in sess.get_inputs()}
            self._output_names = [o.name for o in sess.get_outputs()]

        def prepare(self, run_cfg: _SP_RunCfg, artifacts_dir: Path):  # noqa: ARG002
            return self._sess

        def run(self, prepared: "ort.InferenceSession", inputs: Dict[str, np.ndarray]) -> _SP_BackendRunOut:
            # ORT is strict about unknown input names â†’ filter.
            feeds = {k: v for k, v in inputs.items() if k in self._input_names}
            out_list = prepared.run(None, feeds)
            out_map = {n: np.asarray(a) for n, a in zip(self._output_names, out_list)}
            return _SP_BackendRunOut(outputs=out_map, metrics={})

        def cleanup(self, prepared: object) -> None:  # noqa: ARG002
            # Session lifecycle is managed by the surrounding script.
            return

    class _SP_MergeP2Transfer:
        """Transfer adapter that merges a fixed stage2 base-input dict with stage1 outputs.

        This mirrors the legacy 'build_feeds_p2' behavior: part2 inputs may include both
        cut tensors (from part1 outputs) and some original full-graph inputs.

        It also captures the last interface payload for optional dump_interface.
        """

        def __init__(self, base_p2_inputs: Dict[str, np.ndarray]):
            self._base = _SP_DefaultTransfer()
            self._base_p2_inputs = base_p2_inputs

            self.last_stage1_outputs: Optional[Dict[str, np.ndarray]] = None
            self.last_stage2_inputs: Optional[Dict[str, np.ndarray]] = None
            self.last_bytes: Optional[bytes] = None
            self.last_meta: Optional[_SP_TransferMeta] = None

        def serialize(self, outputs: Dict[str, np.ndarray]) -> Tuple[bytes, _SP_TransferMeta]:
            self.last_stage1_outputs = outputs
            b, meta = self._base.serialize(outputs)
            self.last_bytes, self.last_meta = b, meta
            return b, meta

        def deserialize(self, blob: bytes, meta: _SP_TransferMeta) -> Dict[str, np.ndarray]:
            d = self._base.deserialize(blob, meta)
            merged = dict(self._base_p2_inputs)
            merged.update(d)  # override with cut tensor outputs
            self.last_stage2_inputs = merged
            return merged


DEFAULT_MANIFEST = "__MANIFEST_FILENAME__"
DEFAULT_PROVIDER = "__DEFAULT_PROVIDER__"


# ----------------------------
# Small utilities
# ----------------------------
def _read_json(path: Path) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _write_json(path: Path, obj: dict) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, ensure_ascii=False)


def _now_ts() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())


def _available_providers() -> List[str]:
    try:
        return list(ort.get_available_providers())
    except Exception:
        return []


def _as_ort_opt(v):
    if isinstance(v, bool):
        return "True" if v else "False"
    return str(v)


def _pick_providers(requested: str, available: List[str]) -> List[str]:
    avail = set(available)
    req = (requested or "auto").lower().strip()

    def keep(lst: List[str]) -> List[str]:
        out = [p for p in lst if p in avail]
        if "CPUExecutionProvider" not in out:
            out.append("CPUExecutionProvider")
        return out

    if req in ("auto", "default"):
        if "TensorrtExecutionProvider" in avail:
            return keep(["TensorrtExecutionProvider", "CUDAExecutionProvider", "CPUExecutionProvider"])
        if "CUDAExecutionProvider" in avail:
            return keep(["CUDAExecutionProvider", "CPUExecutionProvider"])
        return ["CPUExecutionProvider"]

    if req in ("tensorrt", "trt"):
        return keep(["TensorrtExecutionProvider", "CUDAExecutionProvider", "CPUExecutionProvider"])
    if req in ("cuda", "gpu"):
        return keep(["CUDAExecutionProvider", "CPUExecutionProvider"])
    if req in ("cpu",):
        return ["CPUExecutionProvider"]

    raise SystemExit(f"Unknown --provider '{requested}'. Expected: auto|tensorrt|cuda|cpu")


@dataclass
class SessionBuildInfo:
    name: str
    model_path: str
    build_seconds: float
    providers_requested: List[str]
    providers_in_use: List[str]


class _Spinner(threading.Thread):
    def __init__(self, prefix: str, every_sec: float = 1.0):
        super().__init__(daemon=True)
        self.prefix = prefix
        self.every_sec = every_sec
        self._tty = sys.stdout.isatty()
        self._stop = threading.Event()
        self.t0 = time.time()
        self._frames = ["|", "/", "-", "\\"]

    def stop(self):
        self._stop.set()

    def run(self):
        if not self._tty:
            # Non-interactive stdout (captured logs): don't emit carriage-returns.
            while not self._stop.is_set():
                time.sleep(self.every_sec)
            return

        i = 0
        while not self._stop.is_set():
            elapsed = int(time.time() - self.t0)
            frame = self._frames[i % len(self._frames)]
            msg = f"\\r{self.prefix} {frame}  elapsed={elapsed}s"
            if self._tty:
                try:
                    sys.stdout.write(msg)
                    sys.stdout.flush()
                except Exception:
                    pass
            i += 1
            time.sleep(self.every_sec)
        if self._tty:
            try:
                sys.stdout.write("\\r" + " " * (len(self.prefix) + 60) + "\\r")
                sys.stdout.flush()
            except Exception:
                pass


def _create_session(
    name: str,
    model_path: Path,
    providers: List[str],
    provider_options: Optional[List[dict]],
    sess_options: ort.SessionOptions,
) -> Tuple[ort.InferenceSession, SessionBuildInfo]:
    trt_note = ''
    try:
        if any('tensorrt' in str(p).lower() for p in (providers or [])):
            trt_note = ' (TensorRT engine build can take minutes)'
    except Exception:
        trt_note = ''
    spin = _Spinner(prefix=f"[init] building '{name}'{trt_note}")
    t0 = time.time()
    spin.start()
    try:
        sess = ort.InferenceSession(
            str(model_path),
            sess_options=sess_options,
            providers=providers,
            provider_options=provider_options,
        )
    finally:
        spin.stop()
        spin.join(timeout=0.2)
    dt = time.time() - t0
    info = SessionBuildInfo(
        name=name,
        model_path=str(model_path),
        build_seconds=float(dt),
        providers_requested=list(providers),
        providers_in_use=list(sess.get_providers()),
    )
    print(f"[init] session '{name}' ready in {dt:.1f}s | providers in use: {info.providers_in_use}")
    return sess, info


# ----------------------------
# Hailo runtime (HEF) helpers
# ----------------------------


def _is_hailo_provider(p: str) -> bool:
    p = (p or "").lower().strip()
    # Hailo DFC uses `hw_arch` strings such as:
    #   hailo8, hailo8l, hailo8r, hailo10, hailo10h
    # Accept all known variants so a benchmark plan can refer to the exact
    # hardware architecture that was used to generate the HEF.
    return p in {"hailo8", "hailo8l", "hailo8r", "hailo10", "hailo10h"}


def _nchw_to_hwc_u8(x: np.ndarray) -> np.ndarray:
    """Convert NCHW/NHWC float/uint input to HWC uint8.

    Used for feeding quantized Hailo inputs.
    Heuristic: if max(x) <= 1.5 => treat as 0..1 and scale to 0..255.
    """
    arr = np.asarray(x)
    if arr.ndim == 4 and arr.shape[0] == 1:
        arr = arr[0]

    # NCHW -> HWC (common for ORT inputs)
    if arr.ndim == 3 and arr.shape[0] in (1, 3) and arr.shape[-1] not in (1, 3):
        arr = np.transpose(arr, (1, 2, 0))

    if arr.ndim != 3:
        raise ValueError(f"Expected 3D image tensor after squeeze/transpose, got shape {arr.shape}")

    if np.issubdtype(arr.dtype, np.floating):
        mx = float(np.nanmax(arr))
        if mx <= 1.5:
            arr = arr * 255.0
        arr = np.clip(arr, 0.0, 255.0).astype(np.uint8)
    else:
        arr = np.clip(arr, 0, 255).astype(np.uint8)
    return arr


def _ensure_frames_dim(x: np.ndarray) -> np.ndarray:
    """Ensure a leading frames dimension for Hailo InferVStreams."""
    if x.ndim == 3:
        return np.expand_dims(x, axis=0)
    return x


def _adapt_tensor(x: np.ndarray, target_shape: Tuple[int, ...]) -> np.ndarray:
    """Best-effort adaptation of a tensor to a target shape.

    Commonly needed when bridging between ORT (often NCHW) and Hailo vstreams
    (often HWC / NHWC).
    """
    arr = np.asarray(x)
    tgt = tuple(int(v) for v in target_shape)

    if tuple(arr.shape) == tgt:
        return arr

    # Remove batch / frames dim.
    if arr.ndim == 4 and arr.shape[0] == 1 and tuple(arr.shape[1:]) == tgt:
        return arr[0]
    if arr.ndim == 4 and arr.shape[0] == 1 and len(tgt) == 3 and tuple(arr.shape[1:]) == tgt:
        return arr[0]

    # Add batch dim (rare).
    if arr.ndim == 3 and len(tgt) == 4 and tgt[0] == 1 and tuple(arr.shape) == tgt[1:]:
        return arr[None, ...]

    # CHW <-> HWC
    if arr.ndim == 3 and len(tgt) == 3:
        # HWC -> CHW
        if arr.shape[-1] == 3 and tgt[0] == 3 and tgt[1] == arr.shape[0] and tgt[2] == arr.shape[1]:
            return np.transpose(arr, (2, 0, 1))
        # CHW -> HWC
        if arr.shape[0] == 3 and tgt[-1] == 3 and tgt[0] == arr.shape[1] and tgt[1] == arr.shape[2]:
            return np.transpose(arr, (1, 2, 0))

    # NCHW <-> NHWC (with batch)
    if arr.ndim == 4 and len(tgt) == 4:
        # NCHW -> NHWC
        if arr.shape[1] == 3 and tgt[-1] == 3 and tgt[0] == arr.shape[0] and tgt[1] == arr.shape[2] and tgt[2] == arr.shape[3]:
            return np.transpose(arr, (0, 2, 3, 1))
        # NHWC -> NCHW
        if arr.shape[-1] == 3 and tgt[1] == 3 and tgt[0] == arr.shape[0] and tgt[2] == arr.shape[1] and tgt[3] == arr.shape[2]:
            return np.transpose(arr, (0, 3, 1, 2))

    # As a last resort, try reshape if the total element count matches.
    try:
        if int(np.prod(arr.shape)) == int(np.prod(tgt)):
            return np.reshape(arr, tgt)
    except Exception:
        pass

    raise ValueError(f"Cannot adapt tensor from shape {tuple(arr.shape)} to target shape {tgt}")


def _map_inputs_to_outputs(expected_inputs: List[str], produced_outputs: List[str]) -> Dict[str, str]:
    """Map expected input names to produced output names.

    Strategy:
    - If names match exactly, use that.
    - If only one input and one output exist, map them.
    - Otherwise, map by suffix match or by order as a fallback.
    """
    mapping: Dict[str, str] = {}

    prod_set = set(produced_outputs)
    for inp in expected_inputs:
        if inp in prod_set:
            mapping[inp] = inp

    if len(mapping) == len(expected_inputs):
        return mapping

    if len(expected_inputs) == 1 and len(produced_outputs) == 1:
        return {expected_inputs[0]: produced_outputs[0]}

    # Suffix match fallback
    for inp in expected_inputs:
        if inp in mapping:
            continue
        cands = [o for o in produced_outputs if o.endswith(inp) or inp.endswith(o)]
        if len(cands) == 1:
            mapping[inp] = cands[0]

    # Order fallback
    if len(mapping) < len(expected_inputs):
        for i, inp in enumerate(expected_inputs):
            if inp in mapping:
                continue
            if i < len(produced_outputs):
                mapping[inp] = produced_outputs[i]

    return mapping


class HailoSession:
    """Reusable Hailo runtime wrapper around InferVStreams.

    Keeps VDevice + network group + InferVStreams open across runs so that timings
    measure inference, not setup.
    """

    def __init__(self, hef_path: Path, *, quantized_inputs: bool, quantized_outputs: bool):
        self.hef_path = Path(hef_path)
        self.quantized_inputs = bool(quantized_inputs)
        self.quantized_outputs = bool(quantized_outputs)

        try:
            import hailo_platform as hpf  # type: ignore
        except Exception as e:
            raise RuntimeError(
                "Hailo backend requested, but 'hailo_platform' is not importable. "
                "Install HailoRT + Python bindings on the target."
            ) from e

        self._hpf = hpf
        self._vdevice = None
        self._pipe = None
        self._network_group = None
        self._input_names: List[str] = []
        self._output_names: List[str] = []
        self.input_shapes: Dict[str, Tuple[int, ...]] = {}
        self.output_shapes: Dict[str, Tuple[int, ...]] = {}
        self._open()

    @property
    def input_names(self) -> List[str]:
        return list(self._input_names)

    @property
    def output_names(self) -> List[str]:
        return list(self._output_names)

    def _open(self) -> None:
        hpf = self._hpf
        if not self.hef_path.exists():
            raise FileNotFoundError(f"HEF not found: {self.hef_path}")

        hef = hpf.HEF(str(self.hef_path))

        # Keep VDevice open.
        self._vdevice = hpf.VDevice()
        try:
            if hasattr(self._vdevice, "__enter__"):
                self._vdevice.__enter__()
        except Exception:
            pass

        cfg_params = hpf.ConfigureParams.create_from_hef(hef, interface=hpf.HailoStreamInterface.PCIe)
        ngs = self._vdevice.configure(hef, cfg_params)
        if not ngs:
            raise RuntimeError("No network groups returned from VDevice.configure")
        self._network_group = ngs[0]

        # Activate.
        ng_params = self._network_group.create_params()
        self._network_group.activate(ng_params)

        self._input_names = [i.name for i in hef.get_input_vstream_infos()]
        self._output_names = [o.name for o in hef.get_output_vstream_infos()]
        self.input_shapes = {i.name: tuple(i.shape) for i in hef.get_input_vstream_infos()}
        self.output_shapes = {o.name: tuple(o.shape) for o in hef.get_output_vstream_infos()}

        in_params = hpf.InputVStreamParams.make(self._network_group, quantized=self.quantized_inputs, format_type=hpf.FormatType.AUTO)
        out_params = hpf.OutputVStreamParams.make(self._network_group, quantized=self.quantized_outputs, format_type=hpf.FormatType.AUTO)
        self._pipe = hpf.InferVStreams(self._network_group, in_params, out_params)
        self._pipe.__enter__()

    def close(self) -> None:
        try:
            if self._pipe is not None and hasattr(self._pipe, "__exit__"):
                self._pipe.__exit__(None, None, None)
        finally:
            self._pipe = None
        try:
            if self._vdevice is not None and hasattr(self._vdevice, "__exit__"):
                self._vdevice.__exit__(None, None, None)
        finally:
            self._vdevice = None

    def infer(self, inputs: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        if self._pipe is None:
            raise RuntimeError("HailoSession is closed")

        in_dict: Dict[str, np.ndarray] = {}
        for name in self._input_names:
            if name not in inputs:
                raise KeyError(f"Missing Hailo input '{name}'. Provided keys: {list(inputs.keys())}")
            in_dict[name] = _ensure_frames_dim(np.asarray(inputs[name]))

        outs = self._pipe.infer(in_dict)
        out_dict: Dict[str, np.ndarray] = {}
        for k, v in outs.items():
            arr = np.asarray(v)
            # Remove frames dim when present.
            if arr.ndim >= 1 and arr.shape[0] == 1:
                try:
                    arr = np.squeeze(arr, axis=0)
                except Exception:
                    pass
            out_dict[str(k)] = arr
        return out_dict

    def __enter__(self) -> "HailoSession":
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        self.close()


# ----------------------------
# ONNX IO helpers
# ----------------------------
def _get_initializer_names(model: onnx.ModelProto) -> set:
    names = set()
    for t in model.graph.initializer:
        if t.name:
            names.add(t.name)
    for t in getattr(model.graph, "sparse_initializer", []):
        if t.name:
            names.add(t.name)
    return names


def _get_non_initializer_inputs(model: onnx.ModelProto) -> List[onnx.ValueInfoProto]:
    init_names = _get_initializer_names(model)
    return [vi for vi in model.graph.input if vi.name not in init_names]


def _np_dtype_from_onnx(elem_type: int) -> np.dtype:
    # Compatible across ONNX versions (onnx.mapping moved/changed).
    try:
        from onnx import helper as onnx_helper
        return np.dtype(onnx_helper.tensor_dtype_to_np_dtype(elem_type))
    except Exception:
        try:
            tp = onnx.TensorProto
            m = {
                tp.FLOAT: np.float32,
                tp.FLOAT16: np.float16,
                tp.DOUBLE: np.float64,
                tp.INT64: np.int64,
                tp.INT32: np.int32,
                tp.INT16: np.int16,
                tp.INT8: np.int8,
                tp.UINT64: np.uint64,
                tp.UINT32: np.uint32,
                tp.UINT16: np.uint16,
                tp.UINT8: np.uint8,
                tp.BOOL: np.bool_,
            }
            return np.dtype(m.get(int(elem_type), np.float32))
        except Exception:
            return np.float32


def _shape_from_value_info(vi: onnx.ValueInfoProto) -> List[Optional[int]]:
    shape = []
    try:
        dims = vi.type.tensor_type.shape.dim
        for d in dims:
            if d.HasField("dim_value"):
                shape.append(int(d.dim_value))
            else:
                shape.append(None)
    except Exception:
        return []
    return shape


def _parse_shape_override(s: Optional[str]) -> Dict[str, Tuple[int, ...]]:
    out: Dict[str, Tuple[int, ...]] = {}
    if not s:
        return out
    # "name=1x128 other=1x3x640x640"
    for chunk in s.replace(",", " ").split():
        if "=" not in chunk:
            continue
        name, val = chunk.split("=", 1)
        name = name.strip()
        val = val.strip().lower().replace("x", " ")
        dims = [int(x) for x in val.split() if x.strip().isdigit()]
        if name and dims:
            out[name] = tuple(dims)
    return out


def _make_random_inputs(
    model: onnx.ModelProto,
    batch: Optional[int],
    seed: int,
    shape_overrides: Dict[str, Tuple[int, ...]],
    only_names: Optional[List[str]] = None,
) -> Dict[str, np.ndarray]:
    rng = np.random.default_rng(seed)
    out: Dict[str, np.ndarray] = {}
    for vi in _get_non_initializer_inputs(model):
        name = vi.name
        if only_names is not None and name not in only_names:
            continue
        dtype = _np_dtype_from_onnx(vi.type.tensor_type.elem_type)
        shp = _shape_from_value_info(vi)
        if name in shape_overrides:
            shp = list(shape_overrides[name])
        # Replace unknown dims with defaults
        fixed = []
        for i, d in enumerate(shp):
            if d is None or d == 0:
                if i == 0:
                    fixed.append(int(batch) if batch else 1)
                else:
                    fixed.append(1)
            else:
                fixed.append(int(d))
        if not fixed:
            fixed = [int(batch) if batch else 1]
        if dtype == np.bool_:
            out[name] = (rng.random(fixed) > 0.5)
        elif np.issubdtype(dtype, np.integer):
            out[name] = rng.integers(low=0, high=2, size=fixed, dtype=dtype)
        else:
            out[name] = rng.standard_normal(size=fixed).astype(dtype)
    return out


def _save_npz(path: str, arrays: Dict[str, np.ndarray], meta: dict) -> None:
    payload = {k: v for k, v in arrays.items()}
    payload["__meta__"] = np.frombuffer(json.dumps(meta, ensure_ascii=False).encode("utf-8"), dtype=np.uint8)
    np.savez_compressed(path, **payload)


def _load_inputs_npz(path: str) -> Tuple[Dict[str, np.ndarray], Optional[dict]]:
    d = np.load(path, allow_pickle=False)
    out: Dict[str, np.ndarray] = {}
    meta = None
    for k in d.files:
        if k == "__meta__":
            try:
                meta = json.loads(bytes(d[k].tolist()).decode("utf-8"))
            except Exception:
                meta = None
            continue
        out[k] = d[k]
    return out, meta


# ----------------------------
# Image input helper (optional)
# ----------------------------
def _is_probably_image_input(shape: List[Optional[int]]) -> bool:
    # Expect NCHW: [N,3,H,W]
    if len(shape) != 4:
        return False
    c = shape[1]
    return (c == 3) or (c is None)


def _guess_default_image_hw_for_model(model: onnx.ModelProto) -> int:
    """Choose a default H/W for dynamic image inputs.

    - Classification-ish models ("logits"/"prob" outputs, rank-2 outputs) => 224
    - Otherwise => 640
    """
    try:
        out_names = [o.name.lower() for o in model.graph.output]
        if any(("logit" in n) or ("prob" in n) or ("softmax" in n) for n in out_names):
            return 224
        for o in model.graph.output:
            tt = o.type.tensor_type
            if not tt.HasField("shape"):
                continue
            if len(tt.shape.dim) == 2:
                return 224
    except Exception:
        pass
    return 640


def _load_image_as_nchw(
    img_path: Path,
    *,
    target_hw: Tuple[int, int],
    dtype: np.dtype,
    scale: str,
) -> Optional[np.ndarray]:
    if Image is None:
        print("[warn] PIL not available -> cannot load image input; using random inputs.")
        return None
    try:
        img = Image.open(str(img_path)).convert("RGB")
    except Exception as e:
        print(f"[warn] failed to load image '{img_path}': {type(e).__name__}: {e}")
        return None

    w, h = target_hw[1], target_hw[0]
    try:
        img_rs = img.resize((w, h))
    except Exception:
        img_rs = img

    arr = np.array(img_rs, dtype=np.float32)  # HWC, 0..255
    arr = np.transpose(arr, (2, 0, 1))  # CHW
    arr = np.expand_dims(arr, axis=0)  # NCHW

    if scale == "raw":
        # 0..255
        pass
    elif scale == "norm":
        # 0..1
        arr = arr / 255.0
    elif scale == "imagenet":
        # 0..1 -> (x-mean)/std
        arr = arr / 255.0
        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(1, 3, 1, 1)
        std = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 3, 1, 1)
        arr = (arr - mean) / std
    elif scale == "clip":
        # 0..1 -> (x-mean)/std (CLIP)
        arr = arr / 255.0
        mean = np.array([0.48145466, 0.4578275, 0.40821073], dtype=np.float32).reshape(1, 3, 1, 1)
        std = np.array([0.26862954, 0.26130258, 0.27577711], dtype=np.float32).reshape(1, 3, 1, 1)
        arr = (arr - mean) / std
    else:
        # caller handles "auto"
        pass

    if np.issubdtype(dtype, np.floating):
        return arr.astype(dtype, copy=False)
    if np.issubdtype(dtype, np.integer):
        return np.clip(arr, 0, np.iinfo(dtype).max).astype(dtype)
    return arr.astype(dtype, copy=False)


# ----------------------------
# Interface dump
# ----------------------------
def _dump_interface_npz(
    mode: str,
    out_prefix: Optional[str],
    out_dir: Path,
    feeds_left: Dict[str, np.ndarray],
    feeds_right: Dict[str, np.ndarray],
    meta_base: dict,
) -> None:
    def nbytes(d: Dict[str, np.ndarray]) -> int:
        return int(sum(int(v.nbytes) for v in d.values()))

    left_b = nbytes(feeds_left)
    right_b = nbytes(feeds_right)

    def write_one(path: Path, arrays: Dict[str, np.ndarray], extra: dict):
        meta = dict(meta_base)
        meta.update(extra)
        _save_npz(str(path), arrays, meta)

    if out_prefix:
        p = Path(out_prefix)
        if not p.is_absolute():
            p = out_dir / p
        base = p
    else:
        base = out_dir / "interface"

    if mode == "either":
        p_left = base.with_name(base.name + "_left.npz")
        p_right = base.with_name(base.name + "_right.npz")
        write_one(p_left, feeds_left, {"which": "left", "total_nbytes": left_b})
        write_one(p_right, feeds_right, {"which": "right", "total_nbytes": right_b})
        print(f"[dump-interface] wrote {p_left} ({left_b/1024/1024:.3f} MiB)")
        print(f"[dump-interface] wrote {p_right} ({right_b/1024/1024:.3f} MiB)")
        return

    if mode == "min":
        mode = "left" if left_b <= right_b else "right"

    if mode == "left":
        p = base.with_suffix(".npz")
        write_one(p, feeds_left, {"which": "left", "total_nbytes": left_b})
        print(f"[dump-interface] wrote {p} ({left_b/1024/1024:.3f} MiB)")
        return

    if mode == "right":
        p = base.with_suffix(".npz")
        write_one(p, feeds_right, {"which": "right", "total_nbytes": right_b})
        print(f"[dump-interface] wrote {p} ({right_b/1024/1024:.3f} MiB)")
        return

    raise SystemExit(f"Invalid --dump-interface mode '{mode}'")


# ----------------------------
# Benchmark + report
# ----------------------------
def _bench(tag: str, fn, warmup: int, runs: int) -> Tuple[float, float, List[float]]:
    print(f"[{tag}] warmup: {warmup} runs (not measured)")
    for i in range(warmup):
        fn()
    print(f"[{tag}] measured: {runs} runs")
    times = []
    for i in range(runs):
        t0 = time.time()
        fn()
        dt = (time.time() - t0) * 1000.0
        times.append(dt)
        print(f"  run {i+1}/{runs}: {dt:.3f} ms")
    return float(np.mean(times)), float(np.std(times)), times


def _try_write_report_plots(out_dir: Path, report: dict) -> None:
    try:
        import matplotlib.pyplot as plt  # type: ignore
    except Exception:
        print("[viz] matplotlib not available -> skipping validation_report.png/.pdf")
        return

    def _num(x: Any, default: float = 0.0) -> float:
        try:
            if x is None:
                return default
            v = float(x)
            if not np.isfinite(v):
                return default
            return v
        except Exception:
            return default

    if not isinstance(report, dict):
        print('[info] report_plots: report is not a dict; skipping plots')
        return
    t = report.get("timing_ms", {})
    labels = ["full", "part1", "part2", "composed"]
    means = [_num(t.get("full_mean")), _num(t.get("part1_mean")), _num(t.get("part2_mean")), _num(t.get("composed_mean"))]
    stds = [_num(t.get("full_std")), _num(t.get("part1_std")), _num(t.get("part2_std")), _num(t.get("composed_std"))]

    try:
        fig = plt.figure(figsize=(8, 4))
        ax = fig.add_subplot(1, 1, 1)
        xs = np.arange(len(labels))
        ax.bar(xs, means, yerr=stds, capsize=4)
        ax.set_xticks(xs)
        ax.set_xticklabels(labels)
        ax.set_ylabel("Latency (ms)")
        passed = report.get("output_diff", {}).get("passed", True)
        ax.set_title(f"Split benchmark (PASS={passed})")
        ax.grid(True, axis="y", linestyle=":", linewidth=0.5)
        fig.tight_layout()

        p_png = out_dir / "validation_report.png"
        p_pdf = out_dir / "validation_report.pdf"
        fig.savefig(str(p_png), dpi=150)
        fig.savefig(str(p_pdf))
        plt.close(fig)
        print(f"Wrote {p_png} and .pdf")
        report.setdefault("artifacts", {})
        report["artifacts"]["validation_report_png"] = str(p_png.name)
        report["artifacts"]["validation_report_pdf"] = str(p_pdf.name)
    except Exception as e:
        print(f"[viz] failed to write report plots: {type(e).__name__}: {e}")


# ----------------------------
# YOLO decode + draw (optional)
# ----------------------------
_COCO80 = [
    "person","bicycle","car","motorcycle","airplane","bus","train","truck","boat","traffic light",
    "fire hydrant","stop sign","parking meter","bench","bird","cat","dog","horse","sheep","cow",
    "elephant","bear","zebra","giraffe","backpack","umbrella","handbag","tie","suitcase","frisbee",
    "skis","snowboard","sports ball","kite","baseball bat","baseball glove","skateboard","surfboard","tennis racket","bottle",
    "wine glass","cup","fork","knife","spoon","bowl","banana","apple","sandwich","orange",
    "broccoli","carrot","hot dog","pizza","donut","cake","chair","couch","potted plant","bed",
    "dining table","toilet","tv","laptop","mouse","remote","keyboard","cell phone","microwave","oven",
    "toaster","sink","refrigerator","book","clock","vase","scissors","teddy bear","hair drier","toothbrush",
]

# COCO-91 class list (common for DETR-style models; index 0 is "N/A").
_COCO91 = [
    "N/A",
    "person",
    "bicycle",
    "car",
    "motorcycle",
    "airplane",
    "bus",
    "train",
    "truck",
    "boat",
    "traffic light",
    "fire hydrant",
    "N/A",
    "stop sign",
    "parking meter",
    "bench",
    "bird",
    "cat",
    "dog",
    "horse",
    "sheep",
    "cow",
    "elephant",
    "bear",
    "zebra",
    "giraffe",
    "N/A",
    "backpack",
    "umbrella",
    "N/A",
    "N/A",
    "handbag",
    "tie",
    "suitcase",
    "frisbee",
    "skis",
    "snowboard",
    "sports ball",
    "kite",
    "baseball bat",
    "baseball glove",
    "skateboard",
    "surfboard",
    "tennis racket",
    "bottle",
    "N/A",
    "wine glass",
    "cup",
    "fork",
    "knife",
    "spoon",
    "bowl",
    "banana",
    "apple",
    "sandwich",
    "orange",
    "broccoli",
    "carrot",
    "hot dog",
    "pizza",
    "donut",
    "cake",
    "chair",
    "couch",
    "potted plant",
    "bed",
    "N/A",
    "dining table",
    "N/A",
    "N/A",
    "toilet",
    "N/A",
    "tv",
    "laptop",
    "mouse",
    "remote",
    "keyboard",
    "cell phone",
    "microwave",
    "oven",
    "toaster",
    "sink",
    "refrigerator",
    "N/A",
    "book",
    "clock",
    "vase",
    "scissors",
    "teddy bear",
    "hair drier",
    "toothbrush",
]

# -----------------------------
# Validation helpers (Phase 2+)
# -----------------------------

def _select_validation_mode(user_mode: str, provider: str, output_format: str) -> Tuple[str, List[str]]:
    """Select validation mode.

    Modes:
      - strict_elementwise: elementwise max_abs <= eps gates PASS
      - proxy_detections: decode detections and validate with IoU matching (order-invariant)
      - off: no gating (timing only)
      - auto: default policy:
          * TensorRT => proxy (elementwise is still reported, but does not gate)
          * BN6 detections => proxy
          * else => strict_elementwise
    """
    warnings: List[str] = []
    m = (user_mode or "auto").strip().lower()

    if m == "auto":
        if provider == "tensorrt":
            mode = "proxy_detections"
        elif output_format == "bn6_detections":
            mode = "proxy_detections"
        else:
            mode = "strict_elementwise"
    elif m in ("strict_elementwise", "proxy_detections", "off"):
        mode = m
    else:
        mode = "auto"
        warnings.append(f"unknown_validation_mode={user_mode}")
        return _select_validation_mode("auto", provider, output_format)

    # If proxy is requested but we don't know how to decode detections, fallback.
    if mode == "proxy_detections" and output_format not in ("multiscale_head", "bn6_detections"):
        warnings.append("proxy_validation_unavailable_output_format")
        if provider == "tensorrt" and m == "auto":
            mode = "off"
            warnings.append("validation_mode_fallback=off")
        else:
            mode = "strict_elementwise"
            warnings.append("validation_mode_fallback=strict_elementwise")
    return mode, warnings


def _boxes_iou_xyxy(a: np.ndarray, b: np.ndarray) -> float:
    # a,b: [x1,y1,x2,y2]
    ax1, ay1, ax2, ay2 = float(a[0]), float(a[1]), float(a[2]), float(a[3])
    bx1, by1, bx2, by2 = float(b[0]), float(b[1]), float(b[2]), float(b[3])
    inter_x1 = max(ax1, bx1)
    inter_y1 = max(ay1, by1)
    inter_x2 = min(ax2, bx2)
    inter_y2 = min(ay2, by2)
    iw = max(0.0, inter_x2 - inter_x1)
    ih = max(0.0, inter_y2 - inter_y1)
    inter = iw * ih
    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)
    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)
    union = area_a + area_b - inter
    if union <= 0.0:
        return 0.0
    return inter / union


def _proxy_validate_detections(
    dets_ref: List[dict],
    dets_out: List[dict],
    match_iou_thr: float = 0.5,
    pass_match_ratio_thr: float = 0.7,
    pass_mean_iou_thr: float = 0.5,
) -> Tuple[Dict[str, Any], bool]:
    """Order-invariant proxy validation for detection outputs.

    Greedy matching by class_id + IoU.
    """
    n_ref = int(len(dets_ref or []))
    n_out = int(len(dets_out or []))

    if n_ref == 0 and n_out == 0:
        metrics = dict(
            n_ref=0,
            n_out=0,
            matched=0,
            match_ratio=1.0,
            mean_iou_matched=1.0,
        )
        return metrics, True

    # Build arrays for faster access
    ref = dets_ref or []
    out = dets_out or []

    ref_order = sorted(range(n_ref), key=lambda i: float(ref[i].get("score", 0.0)), reverse=True)
    used_out = set()
    ious: List[float] = []
    matched = 0

    for ri in ref_order:
        r = ref[ri]
        r_cls = int(r.get("class_id", -1))
        r_box = np.array([r.get("x1", 0.0), r.get("y1", 0.0), r.get("x2", 0.0), r.get("y2", 0.0)], dtype=np.float32)

        best_j = None
        best_iou = 0.0
        for oj in range(n_out):
            if oj in used_out:
                continue
            o = out[oj]
            if int(o.get("class_id", -2)) != r_cls:
                continue
            o_box = np.array([o.get("x1", 0.0), o.get("y1", 0.0), o.get("x2", 0.0), o.get("y2", 0.0)], dtype=np.float32)
            iou = _boxes_iou_xyxy(r_box, o_box)
            if iou > best_iou:
                best_iou = iou
                best_j = oj
        if best_j is not None and best_iou >= float(match_iou_thr):
            used_out.add(best_j)
            matched += 1
            ious.append(float(best_iou))

    match_ratio = float(matched) / float(max(n_ref, 1))
    mean_iou = float(np.mean(ious)) if ious else 0.0

    proxy_pass = (match_ratio >= float(pass_match_ratio_thr)) and (mean_iou >= float(pass_mean_iou_thr))
    metrics = dict(
        n_ref=n_ref,
        n_out=n_out,
        matched=int(matched),
        match_ratio=match_ratio,
        mean_iou_matched=mean_iou,
    )
    return metrics, bool(proxy_pass)


def _extract_detections_for_proxy(
    output_format: str,
    output_names: List[str],
    outputs: List[np.ndarray],
    img_hw: Tuple[int, int],
    yolo_conf: float,
    yolo_iou: float,
    yolo_max_det: int,
    det_conf: float,
    det_iou: float,
    det_max_det: int,
    labels: Optional[List[str]] = None,
) -> Tuple[List[dict], List[str]]:
    warnings: List[str] = []
    try:
        if output_format == "multiscale_head":
            # YOLOv7-style multiscale head
            dets = _decode_yolo(
                outs=outputs,
                img_hw=img_hw,
                conf_thr=float(yolo_conf),
                iou_thr=float(yolo_iou),
                max_det=int(yolo_max_det),
                labels=labels,
            )
            return dets, warnings

        if output_format == "bn6_detections":
            labels_dict = {i: labels[i] for i in range(len(labels))} if labels else None
            dets, w = _decode_bn6(
                output_names=output_names,
                outputs=outputs,
                img_hw=img_hw,
                conf_thr=float(det_conf),
                iou_thr=float(det_iou),
                max_det=int(det_max_det),
                labels=labels_dict,
            )
            warnings.extend(w)
            return dets, warnings

        return [], ["proxy_unavailable_output_format"]
    except Exception as e:
        return [], [f"proxy_decode_exception={type(e).__name__}: {e}"]




def _sigmoid(x: np.ndarray) -> np.ndarray:
    """Numerically stable sigmoid.

    Why:
      Using ``1/(1+exp(-x))`` directly can overflow for large negative ``x``
      (because ``-x`` becomes a large positive number). In practice this shows up
      as ``RuntimeWarning: overflow encountered in exp`` and can also introduce
      NaNs downstream.

    This implementation avoids overflow by splitting on the sign of ``x``:
      * x >= 0:  1 / (1 + exp(-x))     (safe because -x <= 0)
      * x <  0:  exp(x) / (1 + exp(x)) (safe because x < 0)

    Notes:
      We compute in float32 for speed/stability and return float32.
    """

    x_arr = np.asarray(x)
    x_f = x_arr.astype(np.float32, copy=False)

    out = np.empty_like(x_f, dtype=np.float32)
    pos = x_f >= 0

    # x >= 0 branch
    out[pos] = 1.0 / (1.0 + np.exp(-x_f[pos]))

    # x < 0 branch
    exp_x = np.exp(x_f[~pos])
    out[~pos] = exp_x / (1.0 + exp_x)

    return out


def _is_yolo_multiscale(outputs: List[np.ndarray]) -> bool:
    if len(outputs) != 3:
        return False
    for o in outputs:
        if not isinstance(o, np.ndarray):
            return False
        if o.ndim != 5:
            return False
        if o.shape[1] != 3:
            return False
        if o.shape[-1] < 6:
            return False
    return True


# Default YOLOv5/YOLOv7 anchors for 640 input (works well for many exported yolov7 models)
_YOLO_ANCHORS = [
    [(12, 16), (19, 36), (40, 28)],
    [(36, 75), (76, 55), (72, 146)],
    [(142, 110), (192, 243), (459, 401)],
]


def _nms(boxes: np.ndarray, scores: np.ndarray, iou_thr: float, max_det: int) -> List[int]:
    # boxes: Nx4 (x1,y1,x2,y2)
    if boxes.size == 0:
        return []
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]
    areas = (x2 - x1 + 1e-6) * (y2 - y1 + 1e-6)
    order = scores.argsort()[::-1]
    keep: List[int] = []
    while order.size > 0 and len(keep) < max_det:
        i = int(order[0])
        keep.append(i)
        if order.size == 1:
            break
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])
        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h
        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
        inds = np.where(iou <= iou_thr)[0]
        order = order[inds + 1]
    return keep


def _decode_yolo(
    outs: List[np.ndarray],
    img_hw: Tuple[int, int],
    conf_thr: float,
    iou_thr: float,
    max_det: int,
) -> List[dict]:
    # outs: list of 3 arrays, each (B,3,H,W,5+nc)
    H_img, W_img = img_hw
    # Sort by grid size desc to match anchor groups
    outs_sorted = sorted(outs, key=lambda a: int(a.shape[2]) * int(a.shape[3]), reverse=True)

    all_boxes = []
    all_scores = []
    all_cls = []

    for si, p in enumerate(outs_sorted):
        p = p.astype(np.float32, copy=False)
        b, na, gh, gw, ch = p.shape
        nc = ch - 5
        if nc <= 0:
            continue

        # stride inferred from input size / grid size
        stride_w = W_img / float(gw)
        stride_h = H_img / float(gh)
        stride = float((stride_w + stride_h) * 0.5)

        anchors = np.array(_YOLO_ANCHORS[min(si, len(_YOLO_ANCHORS)-1)], dtype=np.float32).reshape(1, na, 1, 1, 2)

        yv, xv = np.meshgrid(np.arange(gh, dtype=np.float32), np.arange(gw, dtype=np.float32), indexing="ij")
        grid = np.stack((xv, yv), axis=-1).reshape(1, 1, gh, gw, 2)

        xy = _sigmoid(p[..., 0:2]) * 2.0 - 0.5
        wh = (_sigmoid(p[..., 2:4]) * 2.0) ** 2
        obj = _sigmoid(p[..., 4:5])
        cls_scores = _sigmoid(p[..., 5:])

        cls_id = np.argmax(cls_scores, axis=-1).astype(np.int32)
        cls_max = np.max(cls_scores, axis=-1, keepdims=True)
        conf = (obj * cls_max).squeeze(-1)

        # Filter
        mask = conf > conf_thr
        if not np.any(mask):
            continue

        xy = (xy + grid) * stride
        wh = wh * anchors

        # boxes in xywh -> xyxy
        x = xy[..., 0]
        y = xy[..., 1]
        w = wh[..., 0]
        h = wh[..., 1]
        x1 = x - w / 2.0
        y1 = y - h / 2.0
        x2 = x + w / 2.0
        y2 = y + h / 2.0

        sel_x1 = x1[mask]
        sel_y1 = y1[mask]
        sel_x2 = x2[mask]
        sel_y2 = y2[mask]
        sel_conf = conf[mask]
        sel_cls = cls_id[mask]

        boxes = np.stack([sel_x1, sel_y1, sel_x2, sel_y2], axis=1)
        all_boxes.append(boxes)
        all_scores.append(sel_conf)
        all_cls.append(sel_cls)

    if not all_boxes:
        return []

    boxes = np.concatenate(all_boxes, axis=0)
    scores = np.concatenate(all_scores, axis=0)
    cls_ids = np.concatenate(all_cls, axis=0)

    # NMS (class-agnostic for simplicity)
    keep = _nms(boxes, scores, iou_thr=iou_thr, max_det=max_det)
    dets = []
    for i in keep:
        c = int(cls_ids[i])
        dets.append(
            {
                "x1": float(boxes[i, 0]),
                "y1": float(boxes[i, 1]),
                "x2": float(boxes[i, 2]),
                "y2": float(boxes[i, 3]),
                "score": float(scores[i]),
                "class_id": c,
                "class_name": _COCO80[c] if 0 <= c < len(_COCO80) else str(c),
            }
        )
    return dets


def _yolo_plausibility(outputs: Sequence[np.ndarray], img_hw: Tuple[int, int], conf_thr: float, iou_thr: float, max_det: int) -> Tuple[int, float]:
    """Heuristic plausibility for YOLO outputs.

    Used only for *auto* image scaling (norm vs raw). This must never crash.
    If decoding or scoring fails, we return a very low score so the other option
    can win.

    Returns:
        (n, score)
    """

    try:
        dets = _decode_yolo(outputs, img_hw=img_hw, conf_thr=conf_thr, iou_thr=iou_thr, max_det=max_det)

        # Some forks might return numpy arrays or tuples instead of dicts.
        scores: List[float] = []

        if dets is None:
            return 0, -1e9

        if isinstance(dets, np.ndarray):
            # Expect shape [N, ...] with score at col 4 if present.
            try:
                n = int(dets.shape[0])
            except Exception:
                n = 0
            try:
                if dets.ndim >= 2 and dets.shape[1] > 4:
                    scores_arr = dets[:, 4]
                else:
                    scores_arr = dets.reshape(-1)
                avg = float(np.mean(scores_arr)) if scores_arr.size else 0.0
            except Exception:
                avg = 0.0
            # Prefer moderate number of detections and reasonable confidence
            penalty = 0.0
            if n > 200:
                penalty += (n - 200) * 5.0
            if n < 1:
                penalty += 100.0
            penalty += abs(n - 10) * 1.0
            return n, (avg * 100.0 - penalty)

        # Best-effort conversion to list
        if not isinstance(dets, (list, tuple)):
            dets = list(dets) if hasattr(dets, "__iter__") else []

        if not dets:
            return 0, -1e9

        for d in dets:
            if isinstance(d, dict):
                s = d.get("score", d.get("conf", 0.0))
                try:
                    scores.append(float(s))
                except Exception:
                    scores.append(0.0)
                continue

            if isinstance(d, (list, tuple, np.ndarray)):
                try:
                    arr = np.asarray(d).reshape(-1)
                    if arr.size >= 5:
                        scores.append(float(arr[4]))
                    elif arr.size:
                        scores.append(float(arr[-1]))
                    else:
                        scores.append(0.0)
                except Exception:
                    scores.append(0.0)
                continue

            try:
                scores.append(float(getattr(d, "score", 0.0)))
            except Exception:
                scores.append(0.0)

        n = len(scores)
        avg = float(np.mean(scores)) if n else 0.0

        penalty = 0.0
        if n > 200:
            penalty += (n - 200) * 5.0
        if n < 1:
            penalty += 100.0
        penalty += abs(n - 10) * 1.0
        return n, (avg * 100.0 - penalty)

    except Exception as e:
        print(f"[warn] YOLO plausibility probe failed: {e}")
        return 0, -1e9



def _detr_plausibility(dets: List[dict]) -> float:
    """Heuristic for choosing the right image normalization for DETR/YOLOS-like outputs."""
    if not dets:
        return -1e9
    scores = [float(d.get("score", 0.0)) for d in dets]
    scores.sort(reverse=True)
    topk = scores[:10]
    top_mean = float(np.mean(topk)) if topk else 0.0
    # Penalize too many detections (usually indicates wrong preprocessing)
    return top_mean - 0.01 * float(len(dets))


def _draw_dets(img_path: Path, dets: List[dict], out_path: Path, *, img_hw: Tuple[int, int]) -> None:
    if Image is None or ImageDraw is None:
        return
    try:
        img = Image.open(str(img_path)).convert("RGB")
        img = img.resize((img_hw[1], img_hw[0]))
        draw = ImageDraw.Draw(img)
        for d in dets:
            x1, y1, x2, y2 = d["x1"], d["y1"], d["x2"], d["y2"]
            draw.rectangle([x1, y1, x2, y2], outline=(255, 0, 0), width=2)
            txt = f'{d["class_name"]} {d["score"]:.2f}'
            draw.text((x1 + 2, y1 + 2), txt, fill=(255, 0, 0))
        img.save(str(out_path))
    except Exception as e:
        print(f"[viz] failed to draw detections: {type(e).__name__}: {e}")


def _maybe_yolo_viz(
    out_dir: Path,
    img_path: Optional[Path],
    img_hw: Optional[Tuple[int, int]],
    full_out: List[np.ndarray],
    comp_out: List[np.ndarray],
    conf_thr: float,
    iou_thr: float,
    max_det: int,
    output_names: Optional[List[str]] = None,
    labels: Optional[List[str]] = None,
    **_kwargs: Any,
) -> dict:
    info = {"enabled": False}
    if img_path is None or img_hw is None:
        print("[viz] no image input -> skipping YOLO visualization")
        return info
    if not _is_yolo_multiscale(full_out) or not _is_yolo_multiscale(comp_out):
        return info

    info["output_format"] = "multiscale_head"
    print("[viz] YOLO output format: multiscale head (B,3,H,W,...)")
    dets_full = _decode_yolo(full_out, img_hw=img_hw, conf_thr=conf_thr, iou_thr=iou_thr, max_det=max_det)
    dets_comp = _decode_yolo(comp_out, img_hw=img_hw, conf_thr=conf_thr, iou_thr=iou_thr, max_det=max_det)

    p_json_full = out_dir / "detections_full.json"
    p_json_comp = out_dir / "detections_composed.json"
    _write_json(p_json_full, {"image": str(img_path.name), "detections": dets_full})
    _write_json(p_json_comp, {"image": str(img_path.name), "detections": dets_comp})

    p_img_full = out_dir / "detections_full.png"
    p_img_comp = out_dir / "detections_composed.png"
    _draw_dets(img_path, dets_full, p_img_full, img_hw=img_hw)
    _draw_dets(img_path, dets_comp, p_img_comp, img_hw=img_hw)

    if p_img_full.exists():
        print(f"Wrote {p_img_full}")
    if p_img_comp.exists():
        print(f"Wrote {p_img_comp}")
    print(f"Wrote {p_json_full.name} and {p_json_comp.name}")

    info = {
        "enabled": True,
        "image": str(img_path.name),
        "img_hw": list(img_hw),
        "full": {"count": len(dets_full), "json": p_json_full.name, "png": p_img_full.name},
        "composed": {"count": len(dets_comp), "json": p_json_comp.name, "png": p_img_comp.name},
        "params": {"conf_thr": conf_thr, "iou_thr": iou_thr, "max_det": max_det},
    }
    return info




# ----------------------------
# DETR/YOLOS-style postprocess (logits + pred_boxes)
# ----------------------------
def _is_detr_like(output_names: List[str], outputs: List[np.ndarray]) -> bool:
    # Heuristic: DETR-style outputs (logits+boxes) OR Ultralytics-style BN6 detections.
    # Layout A (DETR): logits (..,C) and boxes (..,4)
    # Layout B (BN6):  (B,N,6) or (N,6) = [x1,y1,x2,y2,conf,cls]
    if outputs is None or len(outputs) == 0:
        return False

    # Layout B: single output already contains detections
    for a in outputs:
        s = getattr(a, "shape", ())
        if len(s) in (2, 3) and s[-1] == 6:
            return True

    # Layout A needs at least two outputs
    if len(outputs) < 2:
        return False

    names = [str(n) for n in output_names]
    # Common HF/DETR names
    if ("logits" in names and "pred_boxes" in names):
        return True
    # Shape-based fallback
    shapes = [getattr(a, "shape", ()) for a in outputs]
    has_boxes = any(len(s) >= 2 and s[-1] == 4 for s in shapes)
    has_logits = any(len(s) >= 2 and s[-1] >= 10 for s in shapes)  # num classes usually >= 10
    return bool(has_boxes and has_logits)


def _is_bn6_detections(outputs: List[np.ndarray]) -> bool:
    # Ultralytics/YOLOv10-style detections: (B,N,6) or (N,6)
    if outputs is None:
        return False
    for a in outputs:
        if a is None or not hasattr(a, "shape"):
            continue
        if a.ndim in (2, 3) and a.shape[-1] == 6:
            return True
    return False


def _detect_output_format(output_names: List[str], outputs: List[np.ndarray]) -> str:
    # Keep this super defensive â€” it must never crash.
    try:
        if _is_yolo_multiscale(outputs):
            return "multiscale_head"
        if _is_bn6_detections(outputs):
            return "bn6_detections"
        return "unknown"
    except Exception:
        return "unknown"



def _softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:
    x = x.astype(np.float32, copy=False)
    x = x - np.max(x, axis=axis, keepdims=True)
    e = np.exp(x)
    s = np.sum(e, axis=axis, keepdims=True)
    return e / (s + 1e-12)


def _load_labels(path: Optional[str]) -> Optional[List[str]]:
    if not path:
        return None
    try:
        p = Path(path)
        if not p.exists():
            return None
        lines = [ln.strip() for ln in p.read_text(encoding="utf-8", errors="ignore").splitlines()]
        lines = [ln for ln in lines if ln]
        return lines if lines else None
    except Exception:
        return None


# ---- label helpers (auto) ----

def _labels_from_obj(obj: Any) -> Optional[List[str]]:
    """Convert a label container (list/dict) into a dense list of class names."""
    if obj is None:
        return None
    if isinstance(obj, (list, tuple)):
        out = [str(x) for x in obj]
        return out if out else None
    if isinstance(obj, dict):
        try:
            items = sorted(obj.items(), key=lambda kv: int(kv[0]))
        except Exception:
            items = sorted(obj.items(), key=lambda kv: str(kv[0]))
        out = [str(v) for _, v in items]
        return out if out else None
    return None


def _try_parse_labels_value(text: str) -> Optional[List[str]]:
    """Best-effort parse for labels stored as JSON or Python literals."""
    if not text:
        return None
    s = text.strip()
    if not s:
        return None

    # JSON
    try:
        obj = json.loads(s)
        out = _labels_from_obj(obj)
        if out:
            return out
    except Exception:
        pass

    # Python literal (common for Ultralytics: {0: 'person', ...})
    try:
        obj = ast.literal_eval(s)
        out = _labels_from_obj(obj)
        if out:
            return out
    except Exception:
        pass

    return None


def _extract_labels_from_onnx_metadata(model: Any) -> Optional[List[str]]:
    """Try to extract class names from ONNX metadata_props (common in YOLO exports)."""
    try:
        meta = getattr(model, 'metadata_props', None)
        if not meta:
            return None

        # direct keys
        for entry in meta:
            k = str(getattr(entry, 'key', '')).strip().lower()
            v = str(getattr(entry, 'value', '')).strip()
            if not v:
                continue
            if k in {'names', 'labels', 'classes', 'class_names', 'categories'}:
                out = _try_parse_labels_value(v)
                if out:
                    return out

        # nested blob
        for entry in meta:
            k = str(getattr(entry, 'key', '')).strip().lower()
            v = str(getattr(entry, 'value', '')).strip()
            if not v:
                continue
            if k in {'metadata', 'info', 'model_info'}:
                obj = None
                try:
                    obj = json.loads(v)
                except Exception:
                    try:
                        obj = ast.literal_eval(v)
                    except Exception:
                        obj = None
                if isinstance(obj, dict):
                    for kk in ('names', 'labels', 'classes', 'class_names', 'categories'):
                        if kk in obj:
                            out = _labels_from_obj(obj.get(kk))
                            if out:
                                return out
    except Exception:
        return None

    return None


def _auto_find_labels_file(search_dir: Path) -> Optional[Path]:
    """Look for a labels file near the manifest directory (best effort)."""
    candidates = [
        'labels.txt',
        'classes.txt',
        'class_names.txt',
        'coco.names',
        'coco.txt',
    ]
    for d in [search_dir, search_dir.parent, search_dir.parent.parent]:
        for name in candidates:
            p = d / name
            if p.is_file():
                return p
    return None



def _decode_bn6(
    output_names: List[str],
    outputs: List[np.ndarray],
    img_hw: Tuple[int, int],
    conf_thr: float = 0.25,
    iou_thr: float = 0.45,
    max_det: int = 300,
    labels: Optional[Dict[int, str]] = None,
) -> Tuple[List[dict], List[str]]:
    """Decode Ultralytics/YOLOv10-style detections tensor.

    Expected layout: (B,N,6) or (N,6) with columns:
      [x1, y1, x2, y2, conf, cls]

    Robustness:
      - Supports normalized coords (0..1) and absolute coords (pixels)
      - Heuristically falls back to (x,y,w,h) or (cx,cy,w,h) if values look ambiguous
      - Never raises: returns ([], [warning,...]) on failure
    """
    warnings: List[str] = []
    try:
        H, W = int(img_hw[0]), int(img_hw[1])

        dets_arr = None
        for a in outputs:
            if a is None or not hasattr(a, "shape"):
                continue
            if a.ndim == 3 and a.shape[-1] == 6:
                dets_arr = a[0]
                break
            if a.ndim == 2 and a.shape[-1] == 6:
                dets_arr = a
                break

        if dets_arr is None:
            return [], ["bn6_not_found"]

        dets_arr = np.asarray(dets_arr)
        if dets_arr.size == 0:
            return [], warnings

        raw = dets_arr[:, 0:4].astype(np.float32, copy=False)
        scores = dets_arr[:, 4].astype(np.float32, copy=False)
        cls = dets_arr[:, 5].astype(np.float32, copy=False)

        # Normalized vs absolute coordinate heuristic
        coords_max = float(np.nanmax(raw)) if raw.size else 0.0
        normalized = coords_max <= 1.01
        bw = 1.0 if normalized else float(W)
        bh = 1.0 if normalized else float(H)

        def _candidate_boxes(fmt: str):
            if fmt == "xyxy":
                x1 = np.minimum(raw[:, 0], raw[:, 2])
                y1 = np.minimum(raw[:, 1], raw[:, 3])
                x2 = np.maximum(raw[:, 0], raw[:, 2])
                y2 = np.maximum(raw[:, 1], raw[:, 3])
            elif fmt == "tlwh":
                x1 = raw[:, 0]
                y1 = raw[:, 1]
                x2 = raw[:, 0] + raw[:, 2]
                y2 = raw[:, 1] + raw[:, 3]
            elif fmt == "cxcywh":
                x1 = raw[:, 0] - raw[:, 2] / 2.0
                y1 = raw[:, 1] - raw[:, 3] / 2.0
                x2 = raw[:, 0] + raw[:, 2] / 2.0
                y2 = raw[:, 1] + raw[:, 3] / 2.0
            else:
                raise ValueError(fmt)

            w = x2 - x1
            h = y2 - y1
            valid = (w > 0) & (h > 0)

            # bounds check with small margin (boxes can exceed image a bit)
            mw = 0.05 * bw
            mh = 0.05 * bh
            inb = (x1 >= -mw) & (y1 >= -mh) & (x2 <= bw + mw) & (y2 <= bh + mh)

            score = float(np.mean(valid)) + float(np.mean(inb))
            return score, x1, y1, x2, y2

        cands = {}
        for fmt in ("xyxy", "tlwh", "cxcywh"):
            try:
                cands[fmt] = _candidate_boxes(fmt)
            except Exception:
                pass

        if not cands:
            return [], ["bn6_decode_failed"]

        best_fmt = max(cands.items(), key=lambda kv: kv[1][0])[0]
        if best_fmt != "xyxy":
            warnings.append(f"ambiguous_box_format_fallback={best_fmt}")

        _, x1, y1, x2, y2 = cands[best_fmt]
        boxes = np.stack([x1, y1, x2, y2], axis=1).astype(np.float32)

        if normalized:
            boxes[:, [0, 2]] *= float(W)
            boxes[:, [1, 3]] *= float(H)

        # Clip and enforce ordering
        boxes[:, 0] = np.clip(boxes[:, 0], 0, W - 1)
        boxes[:, 2] = np.clip(boxes[:, 2], 0, W - 1)
        boxes[:, 1] = np.clip(boxes[:, 1], 0, H - 1)
        boxes[:, 3] = np.clip(boxes[:, 3], 0, H - 1)

        x1c = np.minimum(boxes[:, 0], boxes[:, 2])
        y1c = np.minimum(boxes[:, 1], boxes[:, 3])
        x2c = np.maximum(boxes[:, 0], boxes[:, 2])
        y2c = np.maximum(boxes[:, 1], boxes[:, 3])
        boxes = np.stack([x1c, y1c, x2c, y2c], axis=1).astype(np.float32)

        m = scores >= float(conf_thr)
        boxes = boxes[m]
        scores_f = scores[m]
        cls_f = cls[m]
        if boxes.shape[0] == 0:
            return [], warnings

        keep = _nms(boxes, scores_f, iou_thr=float(iou_thr), max_det=int(max_det))
        boxes = boxes[keep]
        scores_f = scores_f[keep]
        cls_f = cls_f[keep]

        # Sort by score desc
        order = np.argsort(-scores_f)
        boxes = boxes[order]
        scores_f = scores_f[order]
        cls_f = cls_f[order]

        # Auto default label set (best effort) if none provided.
        # This primarily helps for COCO-trained models that don't ship label metadata.
        if labels is None:
            try:
                max_c = int(np.max(cls_f)) if cls_f.size else -1
            except Exception:
                max_c = -1
            if 0 <= max_c < len(_COCO80):
                labels = {i: n for i, n in enumerate(_COCO80)}
                warnings.append("labels_default=coco80")
            elif 0 <= max_c < len(_COCO91):
                labels = {i: n for i, n in enumerate(_COCO91)}
                warnings.append("labels_default=coco91")

        out: List[dict] = []
        for i in range(boxes.shape[0]):
            cid = int(cls_f[i])
            out.append(
                dict(
                    x1=float(boxes[i, 0]),
                    y1=float(boxes[i, 1]),
                    x2=float(boxes[i, 2]),
                    y2=float(boxes[i, 3]),
                    score=float(scores_f[i]),
                    class_id=cid,
                    class_name=(labels.get(cid, str(cid)) if labels else str(cid)),
                )
            )
        return out, warnings
    except Exception as e:
        return [], [f"bn6_decode_exception={type(e).__name__}: {e}"]


def _decode_detr(
    output_names: List[str],
    outputs: List[np.ndarray],
    img_hw: Tuple[int, int],
    conf_thr: float,
    iou_thr: float,
    max_det: int,
    labels: Optional[List[str]] = None,
) -> List[dict]:
    """
    Decode end-to-end detector outputs.

    Supported layouts:
      A) logits (B,Q,C)/(Q,C) + boxes (B,Q,4)/(Q,4)
         - If C == (#labels + 1) -> DETR-style softmax with background class (NMS optional)
         - Else -> sigmoid / already-probabilities without background (RT-DETR / YOLOv10/YOLO26 end2end)
                This path is NMS-free by design (we keep top-k only).

      B) dets (B,Q,6)/(Q,6) where last dim is [x1,y1,x2,y2,score,cls]
         (common Ultralytics end-to-end ONNX export for YOLOv10/YOLO26).

    Boxes may be cxcywh or xyxy, normalized or absolute.
    Returned detections are in xyxy pixel coordinates (relative to the input image).
    """
    H, W = img_hw

    name2 = {n: outputs[i] for i, n in enumerate(output_names)}

    # --- Layout B: (Q,6) / (B,Q,6) ---
    det6 = None
    for n, a in name2.items():
        if isinstance(a, np.ndarray) and a.ndim in (2, 3) and a.shape[-1] == 6:
            det6 = a
            break
    if det6 is not None:
        # Ultralytics/YOLOv10-style detections already materialized as [x1,y1,x2,y2,conf,cls]
        labels_dict = {i: labels[i] for i in range(len(labels))} if labels else None
        dets, _warn = _decode_bn6(
            output_names,
            outputs,
            img_hw=img_hw,
            conf_thr=conf_thr,
            iou_thr=iou_thr,
            max_det=max_det,
            labels=labels_dict,
        )
        return dets

    # --- Layout A: logits + boxes ---
    logits = None
    boxes = None
    for n, a in name2.items():
        if not isinstance(a, np.ndarray):
            continue
        if a.ndim in (2, 3) and a.shape[-1] == 4:
            boxes = a
        elif a.ndim in (2, 3) and a.shape[-1] >= 10:
            logits = a

    if logits is None or boxes is None:
        return []

    l = logits[0] if logits.ndim == 3 else logits  # (Q,C)
    b = boxes[0] if boxes.ndim == 3 else boxes      # (Q,4)

    if l.ndim != 2 or b.ndim != 2 or b.shape[1] != 4:
        return []

    Q, C = l.shape

    # Best-effort label defaults when none were provided/found.
    # Many transformer detectors export COCO logits but omit class-name metadata.
    if labels is None:
        if C - 1 == len(_COCO80):
            labels = list(_COCO80)
        elif C - 1 == len(_COCO91):
            labels = list(_COCO91)
        elif C == len(_COCO80):
            labels = list(_COCO80)
        elif C == len(_COCO91):
            labels = list(_COCO91)

    # Decide whether there is an explicit background class (DETR style).
    has_background = False
    if labels is not None and C == (len(labels) + 1):
        has_background = True
    # Common DETR exports (COCO) sometimes have 91+1 or 80+1 classes.
    if labels is None and C in (81, 92):
        has_background = True

    # Compute class probabilities.
    if has_background:
        probs = _softmax(l.astype(np.float32, copy=False), axis=-1)
        probs = probs[:, :-1]  # drop background
    else:
        lf = l.astype(np.float32, copy=False)
        lmin = float(np.nanmin(lf)) if lf.size else 0.0
        lmax = float(np.nanmax(lf)) if lf.size else 0.0
        if 0.0 <= lmin and lmax <= 1.0:
            probs = lf  # already probabilities
        else:
            probs = _sigmoid(lf)

    # Best class per query (NMS-free models rely on low absolute scores for "no object" queries).
    cls = np.argmax(probs, axis=1).astype(np.int64)
    scores = probs[np.arange(Q), cls]

    keep = np.where(scores >= float(conf_thr))[0]
    if keep.size == 0:
        return []

    scores = scores[keep]
    cls = cls[keep]
    b = b[keep].astype(np.float32, copy=False)

    # Keep top-k by score (always).
    if scores.size > max_det:
        order = np.argsort(scores)[::-1][:max_det]
        scores = scores[order]
        cls = cls[order]
        b = b[order]

    # Determine whether boxes are normalized and whether they are xyxy or cxcywh.
    b_max = float(np.nanmax(b)) if b.size else 0.0
    norm = b_max <= 1.5

    # If columns look like x2>=x1 and y2>=y1 for almost all boxes, treat as xyxy.
    frac_xyxy = float(np.mean((b[:, 2] >= b[:, 0]) & (b[:, 3] >= b[:, 1]))) if b.size else 0.0
    box_mode = "xyxy" if frac_xyxy > 0.95 else "cxcywh"

    if norm:
        if box_mode == "xyxy":
            x1 = b[:, 0] * W
            y1 = b[:, 1] * H
            x2 = b[:, 2] * W
            y2 = b[:, 3] * H
        else:
            cx = b[:, 0] * W
            cy = b[:, 1] * H
            bw = b[:, 2] * W
            bh = b[:, 3] * H
            x1 = cx - 0.5 * bw
            y1 = cy - 0.5 * bh
            x2 = cx + 0.5 * bw
            y2 = cy + 0.5 * bh
    else:
        if box_mode == "xyxy":
            x1, y1, x2, y2 = b[:, 0], b[:, 1], b[:, 2], b[:, 3]
        else:
            cx, cy, bw, bh = b[:, 0], b[:, 1], b[:, 2], b[:, 3]
            x1 = cx - 0.5 * bw
            y1 = cy - 0.5 * bh
            x2 = cx + 0.5 * bw
            y2 = cy + 0.5 * bh

    x1 = np.clip(x1, 0.0, W - 1.0)
    y1 = np.clip(y1, 0.0, H - 1.0)
    x2 = np.clip(x2, 0.0, W - 1.0)
    y2 = np.clip(y2, 0.0, H - 1.0)
    # ensure ordering
    x1_, x2_ = np.minimum(x1, x2), np.maximum(x1, x2)
    y1_, y2_ = np.minimum(y1, y2), np.maximum(y1, y2)
    x1, x2, y1, y2 = x1_, x2_, y1_, y2_

    # Apply NMS only for explicit-background DETR style.
    if has_background and scores.size > 0 and float(iou_thr) < 0.999:
        keep_idx = _nms(np.stack([x1, y1, x2, y2], axis=1), scores, iou_thr=float(iou_thr), max_det=max_det)
        x1 = x1[keep_idx]
        y1 = y1[keep_idx]
        x2 = x2[keep_idx]
        y2 = y2[keep_idx]
        scores = scores[keep_idx]
        cls = cls[keep_idx]

    dets = []
    for i in range(scores.shape[0]):
        dets.append({
            "x1": float(x1[i]),
            "y1": float(y1[i]),
            "x2": float(x2[i]),
            "y2": float(y2[i]),
            "score": float(scores[i]),
            "class_id": int(cls[i]),
            "class_name": labels[int(cls[i])] if labels and 0 <= int(cls[i]) < len(labels) else f"cls{int(cls[i])}",
        })
    return dets

def _maybe_detr_viz(
    out_dir: Path,
    img_path: Optional[Path],
    img_hw: Optional[Tuple[int, int]],
    output_names: List[str],
    full_out: List[np.ndarray],
    comp_out: List[np.ndarray],
    conf_thr: float,
    iou_thr: float,
    max_det: int,
    labels: Optional[List[str]],
) -> dict:
    info = {"enabled": False}
    if img_path is None or img_hw is None:
        print("[viz] no image input -> skipping DETR visualization")
        return info
    if not _is_detr_like(output_names, full_out) or not _is_detr_like(output_names, comp_out):
        return info

    fmt = "bn6_detections" if any(
        (isinstance(a, np.ndarray) and a.ndim in (2, 3) and a.shape[-1] == 6) for a in full_out
    ) else "logits_boxes"
    info["output_format"] = fmt
    if fmt == "bn6_detections":
        print("[viz] Output format: bn6 detections (B,N,6)")
    else:
        print("[viz] Output format: logits + pred_boxes (B,Q,C + B,Q,4)")
    dets_full = _decode_detr(output_names, full_out, img_hw=img_hw, conf_thr=conf_thr, iou_thr=iou_thr, max_det=max_det, labels=labels)
    dets_comp = _decode_detr(output_names, comp_out, img_hw=img_hw, conf_thr=conf_thr, iou_thr=iou_thr, max_det=max_det, labels=labels)

    p_json_full = out_dir / "detections_full.json"
    p_json_comp = out_dir / "detections_composed.json"
    _write_json(p_json_full, {"image": str(img_path.name), "detections": dets_full})
    _write_json(p_json_comp, {"image": str(img_path.name), "detections": dets_comp})

    p_img_full = out_dir / "detections_full.png"
    p_img_comp = out_dir / "detections_composed.png"
    _draw_dets(img_path, dets_full, p_img_full, img_hw=img_hw)
    _draw_dets(img_path, dets_comp, p_img_comp, img_hw=img_hw)

    if p_img_full.exists():
        print(f"Wrote {p_img_full}")
    if p_img_comp.exists():
        print(f"Wrote {p_img_comp}")
    print(f"Wrote {p_json_full.name} and {p_json_comp.name}")

    info = {
        "enabled": True,
        "image": str(img_path.name),
        "img_hw": list(img_hw),
        "full": {"count": len(dets_full), "json": p_json_full.name, "png": p_img_full.name},
        "composed": {"count": len(dets_comp), "json": p_json_comp.name, "png": p_img_comp.name},
        "params": {"conf_thr": conf_thr, "iou_thr": iou_thr, "max_det": max_det},
    }
    return info


def _pick_classification_logits(output_names: List[str], outs: List[np.ndarray]) -> Optional[np.ndarray]:
    """Pick the most likely classification output (logits/probs)."""
    # Prefer outputs explicitly named like logits/probabilities.
    for i, name in enumerate(output_names):
        if i >= len(outs):
            continue
        a = outs[i]
        if not isinstance(a, np.ndarray):
            continue
        if a.ndim not in (1, 2):
            continue
        n = name.lower()
        if ("logit" in n) or ("prob" in n) or ("softmax" in n) or ("pred" in n):
            return a

    # Otherwise: choose the largest 1D/2D output.
    best = None
    best_c = -1
    for a in outs:
        if not isinstance(a, np.ndarray):
            continue
        if a.ndim not in (1, 2):
            continue
        c = int(a.shape[-1]) if a.shape else 0
        if c > best_c:
            best_c = c
            best = a
    return best


def _softmax1d(x: np.ndarray) -> np.ndarray:
    x = x.astype(np.float32)
    # Stabilize; tolerate NaNs.
    m = np.nanmax(x)
    if not np.isfinite(m):
        return np.zeros_like(x, dtype=np.float32)
    z = x - m
    z = np.clip(z, -80.0, 80.0)
    e = np.exp(z)
    s = float(np.nansum(e))
    if not np.isfinite(s) or s <= 0:
        return np.zeros_like(x, dtype=np.float32)
    return (e / s).astype(np.float32)


def _maybe_classification_viz(
    out_dir: Path,
    img_path: Optional[Path],
    output_names: List[str],
    full_out: List[np.ndarray],
    comp_out: List[np.ndarray],
    labels: Optional[List[str]],
    topk: int = 5,
) -> dict:
    """Write simple classification artifacts (top-k + annotated image).

    This is intentionally lightweight: it never raises, and it falls back to numeric
    class indices when labels are unavailable.
    """
    info = {"enabled": False}
    if img_path is None or not img_path.exists():
        return info

    logits_full = _pick_classification_logits(output_names, full_out)
    logits_comp = _pick_classification_logits(output_names, comp_out)
    if logits_full is None or logits_comp is None:
        return info

    def _as_1d(a: np.ndarray) -> Optional[np.ndarray]:
        if a.ndim == 2:
            if a.shape[0] < 1:
                return None
            return a[0]
        if a.ndim == 1:
            return a
        return None

    v_full = _as_1d(logits_full)
    v_comp = _as_1d(logits_comp)
    if v_full is None or v_comp is None:
        return info

    # If no labels were provided but the output looks like ImageNet-1k, use built-in labels.
    if labels is None and v_full.shape[0] == len(IMAGENET1K_LABELS):
        labels = IMAGENET1K_LABELS

    probs_full = _softmax1d(v_full)
    probs_comp = _softmax1d(v_comp)

    def _topk_rows(v: np.ndarray, p: np.ndarray) -> List[dict]:
        k = max(1, min(int(topk), int(v.shape[0])))
        idxs = np.argsort(-p)[:k]
        rows = []
        for r, idx in enumerate(idxs.tolist()):
            lbl = labels[idx] if (labels is not None and 0 <= idx < len(labels)) else str(idx)
            rows.append({
                "rank": int(r + 1),
                "index": int(idx),
                "label": str(lbl),
                "prob": float(p[idx]),
                "logit": float(v[idx]),
            })
        return rows

    rows_full = _topk_rows(v_full, probs_full)
    rows_comp = _topk_rows(v_comp, probs_comp)

    # Always copy the input image used (helps debugging on headless systems).
    try:
        p_in = out_dir / "input_image.png"
        if not p_in.exists():
            Image.open(img_path).convert("RGB").save(p_in)
    except Exception:
        pass

    def _render(rows: List[dict], out_path: Path):
        im = Image.open(img_path).convert("RGB")
        draw = ImageDraw.Draw(im)
        y = 6
        # translucent background for text
        try:
            box_h = 16 * (len(rows) + 1) + 6
            box_w = max(240, int(im.size[0] * 0.45))
            overlay = Image.new("RGBA", im.size, (0, 0, 0, 0))
            od = ImageDraw.Draw(overlay)
            od.rectangle([0, 0, box_w, box_h], fill=(0, 0, 0, 140))
            im = Image.alpha_composite(im.convert("RGBA"), overlay).convert("RGB")
            draw = ImageDraw.Draw(im)
        except Exception:
            pass

        draw.text((6, y), "Top-k predictions", fill=(255, 255, 255))
        y += 18
        for row in rows:
            line = f"{row['rank']}: {row['label']}  (p={row['prob']:.3f})"
            draw.text((6, y), line, fill=(255, 255, 255))
            y += 16
        im.save(out_path)

    p_json_full = out_dir / "classification_full.json"
    p_json_comp = out_dir / "classification_composed.json"
    _write_json(p_json_full, {"image": str(img_path.name), "topk": rows_full})
    _write_json(p_json_comp, {"image": str(img_path.name), "topk": rows_comp})

    p_img_full = out_dir / "classification_full.png"
    p_img_comp = out_dir / "classification_composed.png"
    _render(rows_full, p_img_full)
    _render(rows_comp, p_img_comp)

    print("[viz] Output format: classification logits")
    if p_img_full.exists():
        print(f"Wrote {p_img_full}")
    if p_img_comp.exists():
        print(f"Wrote {p_img_comp}")
    print(f"Wrote {p_json_full.name} and {p_json_comp.name}")

    info = {
        "enabled": True,
        "image": str(img_path.name),
        "full": {"json": p_json_full.name, "png": p_img_full.name},
        "composed": {"json": p_json_comp.name, "png": p_img_comp.name},
        "params": {"topk": int(topk)},
    }
    return info


# ----------------------------
# Main
# ----------------------------
def main() -> int:
    ap = argparse.ArgumentParser(description="Run ORT benchmark for a split ONNX model set.")
    ap.add_argument("--manifest", type=str, default=DEFAULT_MANIFEST, help="Split manifest JSON (default from export).")
    ap.add_argument(
        "--provider",
        type=str,
        default=DEFAULT_PROVIDER,
        choices=["auto", "tensorrt", "cuda", "cpu", "hailo8", "hailo8l", "hailo8r", "hailo10", "hailo10h"],
        help="Backend preference (ORT: auto/tensorrt/cuda/cpu; Hailo: hailo8/hailo8l/hailo8r/hailo10/hailo10h).",
    )
    ap.add_argument("--out-dir", type=str, default="results", help="Output dir for reports (default: results/)")
    ap.add_argument("--warmup", type=int, default=5, help="Warmup runs per benchmark")
    ap.add_argument("--runs", type=int, default=10, help="Measured runs per benchmark")
    ap.add_argument("--eps", type=float, default=1e-4, help="Max-abs threshold for output diff PASS/FAIL")
    ap.add_argument(
        "--validation-mode",
        type=str,
        default="auto",
        choices=["auto", "strict_elementwise", "proxy_detections", "off"],
        help="Validation gating: auto|strict_elementwise|proxy_detections|off. (auto: TRT/BN6=>proxy)",
    )
    ap.add_argument("--seed", type=int, default=0, help="RNG seed for random input generation")
    ap.add_argument("--batch", type=int, default=None, help="Batch dim for random input generation (if dim0 unknown)")
    ap.add_argument("--shape-override", type=str, default=None, help="Override input shapes for random generation, shorthand: name=1x128;other=1x3x640x640")
    ap.add_argument("--inputs-npz", type=str, default=None, help="Load model inputs from an .npz file (keys are input names). Missing inputs are generated.")
    ap.add_argument("--save-inputs-npz", type=str, default=None, help="Save the final full-model input feed dict to an .npz file (includes __meta__).")

    # Image helper (for CV models)
    ap.add_argument("--image", type=str, default="test_image.png", help="Optional image file for CV inputs (default: test_image.png)")
    ap.add_argument("--image-scale", type=str, default="auto", choices=["auto", "norm", "raw", "imagenet", "clip"], help="Image preprocessing for float inputs: raw=0..255, norm=img/255, imagenet=(img/255-mean)/std, clip=CLIP norm, auto=probe (YOLO/DETR)")

    # Visualization
    ap.add_argument(
        "--viz",
        type=str,
        default="auto",
        choices=["auto", "none", "yolo", "detr", "cls"],
        help="Visualization: auto (YOLO/DETR/CLS if detected), yolo, detr, cls, none",
    )
    ap.add_argument("--no-report-plots", action="store_true", help="Disable validation_report.png/.pdf generation")
    ap.add_argument("--yolo-conf", type=float, default=0.25, help="YOLO confidence threshold")
    ap.add_argument("--yolo-iou", type=float, default=0.45, help="YOLO NMS IoU threshold")
    ap.add_argument("--yolo-max-det", type=int, default=200, help="YOLO max detections after NMS")
    ap.add_argument("--detr-conf", type=float, default=0.25, help="DETR/YOLOS confidence threshold")
    ap.add_argument("--detr-iou", type=float, default=0.50, help="DETR/YOLOS NMS IoU threshold")
    ap.add_argument("--detr-max-det", type=int, default=200, help="DETR/YOLOS max detections after NMS")
    ap.add_argument("--labels", type=str, default=None, help="Optional label file (one class name per line) for visualization")

    # Interface dump
    ap.add_argument("--dump-interface", type=str, default=None, choices=["right", "left", "min", "either"], help="Dump interface NPZ(s): right=part2 feed, left=part1 feed, either=both, min=smaller.")
    ap.add_argument("--dump-interface-out", type=str, default=None, help="Output path/prefix for interface NPZ(s). Default: <out-dir>/interface_*.npz")

    # Session options
    ap.add_argument("--build-only", action="store_true", help="Only build sessions/engines, run 1 inference each, exit.")
    ap.add_argument("--ort-log-severity", type=int, default=2, help="ORT log severity (0=verbose,1=info,2=warning,3=error,4=fatal).")

    # TRT options
    ap.add_argument("--trt-cache-dir", type=str, default="trt_cache", help="TensorRT engine cache directory")
    ap.add_argument("--trt-cache", dest="trt_cache", action="store_true", default=True, help="Enable engine cache")
    ap.add_argument("--no-trt-cache", dest="trt_cache", action="store_false", help="Disable engine cache")
    ap.add_argument("--trt-fp16", action="store_true", help="Enable TRT FP16")
    ap.add_argument("--trt-dump-subgraphs", action="store_true", help="Enable TRT subgraph dump (ORT TensorRT EP)")
    ap.add_argument("--trt-fast-build", action="store_true", help="Faster TRT build preset (opt level 2)")

    args = ap.parse_args()

    # Snapshot a JSON-serializable view of the run configuration for reports.
    # Keep it simple: just serialize the argparse namespace.
    run_cfg = dict(vars(args))

    base_dir = Path(__file__).resolve().parent


    # Suite-level benchmark plan (optional).
    # In generated benchmark suites, each case lives in its own subfolder (e.g. suite/b112/),
    # while benchmark_plan.json is typically placed at the suite root (suite/benchmark_plan.json).
    # This is only used for provenance in the JSON report; failure to load must NOT abort the run.
    plan: Optional[dict] = None
    try:
        plan_path = base_dir.parent / "benchmark_plan.json"
        if plan_path.exists():
            plan = _read_json(plan_path)
    except Exception as e:
        print(f"[warn] failed to read suite benchmark_plan.json: {e} (continuing without plan)")
        plan = None

    # Optional class labels for visualization / proxy validation.
    # Must never crash the benchmark runner; on failure we just proceed without labels.
    labels: Optional[List[str]] = None
    if getattr(args, "labels", None):
        labels_path = Path(args.labels)
        if not labels_path.is_absolute():
            labels_path = base_dir / labels_path
        try:
            labels = _load_labels(labels_path)
            print(f"[info] loaded {len(labels)} labels from: {labels_path}")
        except Exception as e:
            print(f"[warn] failed to load labels from {labels_path}: {e} (continuing without labels)")
            labels = None
    manifest_path = Path(args.manifest)
    if not manifest_path.is_absolute():
        manifest_path = base_dir / manifest_path
    if not manifest_path.exists():
        raise SystemExit(f"Manifest not found: {manifest_path}")

    out_dir = Path(args.out_dir)
    if not out_dir.is_absolute():
        out_dir = base_dir / out_dir
    out_dir.mkdir(parents=True, exist_ok=True)

    manifest = _read_json(manifest_path)

    # Manifest schema compatibility
    full_rel = manifest.get("full_model") or manifest.get("full") or manifest.get("model")
    p1_rel = manifest.get("part1_model") or manifest.get("part1") or manifest.get("part1_path")
    p2_rel = manifest.get("part2_model") or manifest.get("part2") or manifest.get("part2_path")
    if not full_rel or not p1_rel or not p2_rel:
        raise SystemExit(
            "Manifest schema not recognized. Expected keys like "
            "'full_model' and ('part1'/'part1_model') and ('part2'/'part2_model'). "
            f"Found keys: {sorted(manifest.keys())}"
        )

    full_path = Path(full_rel)
    p1_path = Path(p1_rel)
    p2_path = Path(p2_rel)
    if not full_path.is_absolute():
        full_path = base_dir / full_path
    if not p1_path.is_absolute():
        p1_path = base_dir / p1_path
    if not p2_path.is_absolute():
        p2_path = base_dir / p2_path

    print(f"[info] manifest: {manifest_path}")
    print(f"[info] full : {full_path.name}")
    print(f"[info] part1: {p1_path.name}")
    print(f"[info] part2: {p2_path.name}")

    # Load only the ONNX skeleton (avoid pulling huge external weights into RAM)
    full_model = onnx.load(str(full_path), load_external_data=False)
    default_hw = _guess_default_image_hw_for_model(full_model)

    # Auto-load labels (for nicer detection visualizations):
    # 1) explicit --labels path
    # 2) ONNX metadata (common in YOLO exports)
    # 3) nearby labels file (labels.txt / classes.txt / coco.names)
    if labels is None:
        labels = _extract_labels_from_onnx_metadata(full_model)
        if labels:
            print(f"[info] labels: {len(labels)} classes (from ONNX metadata)")
        else:
            lp = _auto_find_labels_file(base_dir)
            if lp:
                labels = _load_labels(str(lp))
                if labels:
                    print(f"[info] labels: {len(labels)} classes (from {lp.name})")


    shape_overrides = _parse_shape_override(args.shape_override)

    feeds_full: Dict[str, np.ndarray] = {}
    loaded_inputs_meta = None
    if args.inputs_npz:
        feeds_full, loaded_inputs_meta = _load_inputs_npz(args.inputs_npz)
        print(f"[inputs] loaded {len(feeds_full)} tensor(s) from {args.inputs_npz}")

    required_inputs = [vi.name for vi in _get_non_initializer_inputs(full_model)]
    missing = [n for n in required_inputs if n not in feeds_full]

    # Optional: use image for missing CV input(s)
    img_path = None
    img_hw = None
    img_candidates = None  # type: ignore
    if missing and not args.inputs_npz and args.image:
        p_img = Path(args.image)
        if not p_img.is_absolute():
            p_img = base_dir / p_img
        if p_img.exists():
            # Find first image-like input among missing
            for vi in _get_non_initializer_inputs(full_model):
                if vi.name not in missing:
                    continue
                shp = _shape_from_value_info(vi)
                if vi.name in shape_overrides:
                    shp = list(shape_overrides[vi.name])
                if not _is_probably_image_input(shp):
                    continue
                dtype = _np_dtype_from_onnx(vi.type.tensor_type.elem_type)
                # Determine H,W
                H = int(shp[2]) if shp[2] else int(default_hw)
                W = int(shp[3]) if shp[3] else int(default_hw)
                img_hw = (H, W)
                img_path = p_img
                if args.image_scale == "auto":
                    img_candidates = {
                        "raw": _load_image_as_nchw(p_img, target_hw=img_hw, dtype=dtype, scale="raw"),
                        "norm": _load_image_as_nchw(p_img, target_hw=img_hw, dtype=dtype, scale="norm"),
                        "imagenet": _load_image_as_nchw(p_img, target_hw=img_hw, dtype=dtype, scale="imagenet"),
                        "clip": _load_image_as_nchw(p_img, target_hw=img_hw, dtype=dtype, scale="clip"),
                        "name": vi.name,
                    }
                    # default to norm
                    if img_candidates.get("norm") is not None:
                        feeds_full[vi.name] = img_candidates["norm"]
                        missing.remove(vi.name)
                else:
                    arr = _load_image_as_nchw(p_img, target_hw=img_hw, dtype=dtype, scale=args.image_scale)
                    if arr is not None:
                        feeds_full[vi.name] = arr
                        missing.remove(vi.name)
                if vi.name in feeds_full:
                    print(f"[inputs] using image '{p_img.name}' for input '{vi.name}' shape={list(feeds_full[vi.name].shape)} scale={args.image_scale}")
                    break

    if missing:
        feeds_full.update(
            _make_random_inputs(
                full_model,
                batch=args.batch,
                seed=args.seed,
                shape_overrides=shape_overrides,
                only_names=missing,
            )
        )
        if args.inputs_npz:
            print(f"[inputs] generated {len(missing)} missing input(s): {missing[:8]}{'...' if len(missing)>8 else ''}")

    if args.save_inputs_npz:
        meta = {
            "format": "model_inputs_npz_v1",
            "created": _now_ts(),
            "created_time_unix": time.time(),
            "manifest": str(manifest_path.name),
            "shape_override": args.shape_override,
            "loaded_meta": loaded_inputs_meta,
            "inputs": {k: {"shape": list(v.shape), "dtype": str(v.dtype), "nbytes": int(v.nbytes)} for k, v in feeds_full.items()},
        }
        out_npz = Path(args.save_inputs_npz)
        if not out_npz.is_absolute():
            out_npz = out_dir / out_npz
        _save_npz(str(out_npz), feeds_full, meta)
        print(f"[inputs] wrote {out_npz} (tensors={len(feeds_full)})")

    req_provider = str(args.provider or "auto").lower().strip()
    hailo_hw = req_provider if _is_hailo_provider(req_provider) else ""

    avail = _available_providers()
    print(f"ORT available providers: {avail}")

    # If a Hailo backend is requested, we still use ORT (CPU) for baseline outputs
    # and for graph metadata (names/shapes). Timings are taken from Hailo.
    ort_provider_hint = "cpu" if hailo_hw else req_provider
    providers = _pick_providers(ort_provider_hint, avail)
    if hailo_hw:
        print(f"Using Hailo backend: {hailo_hw} (ORT baseline: {providers})")
    else:
        print(f"Using providers: {providers}")

    sess_options = ort.SessionOptions()
    sess_options.log_severity_level = int(args.ort_log_severity)

    provider_options: Optional[List[dict]] = None
    if "TensorrtExecutionProvider" in providers:
        cache_dir = Path(args.trt_cache_dir)
        if not cache_dir.is_absolute():
            cache_dir = base_dir / cache_dir
        cache_dir.mkdir(parents=True, exist_ok=True)
        print(f"[tensorrt] engine cache dir: {cache_dir}")

        trt_opts = {
            "trt_engine_cache_enable": _as_ort_opt(bool(args.trt_cache)),
            "trt_engine_cache_path": str(cache_dir),
            "trt_timing_cache_enable": _as_ort_opt(bool(args.trt_cache)),
            "trt_timing_cache_path": str(cache_dir / "timing.cache"),
            "trt_fp16_enable": _as_ort_opt(bool(args.trt_fp16)),
            "trt_dump_subgraphs": _as_ort_opt(bool(args.trt_dump_subgraphs)),
        }
        if args.trt_fast_build:
            trt_opts["trt_builder_optimization_level"] = _as_ort_opt(2)
            trt_opts["trt_build_heuristics_enable"] = _as_ort_opt(True)

        provider_options = []
        for p in providers:
            provider_options.append(trt_opts if p == "TensorrtExecutionProvider" else {})

    sess_full, info_full = _create_session("full", full_path, providers, provider_options, sess_options)
    sess_p1, info_p1 = _create_session("part1", p1_path, providers, provider_options, sess_options)
    sess_p2, info_p2 = _create_session("part2", p2_path, providers, provider_options, sess_options)

    # If image_scale=auto, try to pick a sane preprocessing preset.
    if img_candidates is not None and args.image_scale == "auto":
        out_names = [o.name for o in sess_full.get_outputs()]

        def _run_full_with_scale(scale: str):
            feeds = dict(feeds_full)
            feeds[img_candidates["name"]] = img_candidates[scale]
            return sess_full.run(None, feeds)

        # Probe using "norm" once to identify output family (YOLO vs DETR-like vs unknown)
        out_norm = _run_full_with_scale("norm")

        if _is_yolo_multiscale(out_norm):
            print("[auto] Probing YOLO input scaling (norm vs raw)...")
            out_raw = _run_full_with_scale("raw")
            n_norm, p_norm = _yolo_plausibility(out_norm, img_hw, args.yolo_conf, args.yolo_iou, args.yolo_max_det)
            n_raw, p_raw = _yolo_plausibility(out_raw, img_hw, args.yolo_conf, args.yolo_iou, args.yolo_max_det)
            print(f"  image_scale=norm: {n_norm} dets, plausibility={p_norm:.1f}")
            print(f"  image_scale=raw : {n_raw} dets, plausibility={p_raw:.1f}")
            selected = "norm" if p_norm >= p_raw else "raw"
            print(f"[auto] Selected image_scale={selected}.")
            feeds_full[img_candidates["name"]] = img_candidates[selected]

        elif _is_detr_like(out_norm, out_names):
            print("[auto] Probing DETR/YOLOS input normalization (imagenet/norm/raw/clip)...")
            cand_order = ["imagenet", "norm", "raw", "clip"]
            best_scale = None
            best_plaus = -1e9
            for sc in cand_order:
                if sc not in img_candidates:
                    continue
                out_sc = out_norm if sc == "norm" else _run_full_with_scale(sc)
                dets = _decode_detr(
                    out_sc,
                    out_names,
                    img_hw=img_hw,
                    conf_thr=args.viz_conf,
                    iou_thr=args.viz_iou,
                    max_det=args.viz_max_det,
                )
                plaus = _detr_plausibility(dets)
                print(f"  image_scale={sc:8s}: {len(dets)} dets, plausibility={plaus:.3f}")
                if plaus > best_plaus:
                    best_plaus = plaus
                    best_scale = sc
            if best_scale is None:
                best_scale = "norm"
            print(f"[auto] Selected image_scale={best_scale}.")
            feeds_full[img_candidates["name"]] = img_candidates.get(best_scale, img_candidates["norm"])
        else:
            # Unknown outputs: choose a sensible default.
            default_sc = "imagenet" if (int(default_hw) == 224 and "imagenet" in img_candidates) else "norm"
            why = "classification-ish outputs" if default_sc == "imagenet" else "unknown outputs"
            print(f"[auto] image_scale=auto -> defaulting to {default_sc} ({why}).")
            feeds_full[img_candidates["name"]] = img_candidates[default_sc]

    # Build feeds for part1/part2
    p1_inputs = [i.name for i in sess_p1.get_inputs()]
    feeds_p1: Dict[str, np.ndarray] = {}
    missing_p1: List[str] = []
    for name in p1_inputs:
        if name in feeds_full:
            feeds_p1[name] = feeds_full[name]
        else:
            missing_p1.append(name)
    if missing_p1:
        raise SystemExit(f"Missing inputs for part1: {missing_p1}")

    p1_out0 = sess_p1.run(None, feeds_p1)
    p1_map0: Dict[str, np.ndarray] = {o.name: a for o, a in zip(sess_p1.get_outputs(), p1_out0)}

    p2_inputs = [i.name for i in sess_p2.get_inputs()]
    missing_p2 = [n for n in p2_inputs if (n not in p1_map0 and n not in feeds_full)]
    if missing_p2:
        raise SystemExit(f"Missing inputs for part2 (neither cut tensor nor in full inputs): {missing_p2}")

    def build_feeds_p2(p1_map: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        d: Dict[str, np.ndarray] = {}
        for n in p2_inputs:
            d[n] = p1_map[n] if n in p1_map else feeds_full[n]
        return d

    feeds_p2_0 = build_feeds_p2(p1_map0)

    if args.dump_interface:
        meta_base = {
            "format": "split_interface_npz_v1",
            "created": _now_ts(),
            "created_time_unix": time.time(),
            "manifest": str(manifest_path.name),
            "mode": args.dump_interface,
            "provider": args.provider,
            "providers_in_use": {"full": info_full.providers_in_use, "part1": info_p1.providers_in_use, "part2": info_p2.providers_in_use},
            "shape_override": args.shape_override,
            "inputs_npz": args.inputs_npz,
            "note": "NPZ keys are exact ORT input names. '__meta__' contains JSON metadata.",
            "left_inputs": {k: {"shape": list(v.shape), "dtype": str(v.dtype), "nbytes": int(v.nbytes)} for k, v in feeds_p1.items()},
            "right_inputs": {k: {"shape": list(v.shape), "dtype": str(v.dtype), "nbytes": int(v.nbytes)} for k, v in feeds_p2_0.items()},
        }
        _dump_interface_npz(args.dump_interface, args.dump_interface_out, out_dir, feeds_p1, feeds_p2_0, meta_base)

    if args.build_only:
        print("[build-only] running 1 inference(s) to finalize engine builds/caches...")
        _ = sess_full.run(None, feeds_full)
        _ = sess_p1.run(None, feeds_p1)
        _ = sess_p2.run(None, feeds_p2_0)
        report = {
            "created": _now_ts(),
            "manifest": str(manifest_path),
            "sessions": {"full": info_full.__dict__, "part1": info_p1.__dict__, "part2": info_p2.__dict__},
            "providers_available": avail,
            "providers_requested": providers,
            "note": "Build-only mode: no benchmarks/diffs.",
        }
        out_report = out_dir / "build_report.json"
        _write_json(out_report, report)
        print(f"[build-only] wrote {out_report}")
        print("[build-only] done.")
        return 0

    def run_full():
        return sess_full.run(None, feeds_full)

    # ----------------
    # ORT vs Hailo run
    # ----------------

    final_out_names = [o.name for o in sess_full.get_outputs()]

    hailo_p1: Optional[HailoSession] = None
    hailo_p2: Optional[HailoSession] = None

    def _order_out_map(out_map: Dict[str, np.ndarray]) -> List[np.ndarray]:
        # Primary: order like the full graph outputs.
        ordered = [out_map[n] for n in final_out_names if n in out_map]
        if ordered and len(ordered) == len(final_out_names):
            return ordered

        # Secondary: order like the right/part2 outputs.
        alt = [out_map[n] for n in [o.name for o in sess_p2.get_outputs()] if n in out_map]
        if alt:
            return alt

        # Fallback: deterministic order.
        return [out_map[k] for k in sorted(out_map.keys())]

    # Optional debug payload (only populated when using GraphRunner).
    graph_runner_metrics = None

    if hailo_hw:
        # Hailo HEFs are expected at: hailo/<hw_arch>/part{1|2}/compiled.hef
        # where <hw_arch> is e.g. hailo8, hailo8l, hailo10h, ...
        hef_root = base_dir / "hailo" / hailo_hw
        hef_p1 = hef_root / "part1" / "compiled.hef"
        hef_p2 = hef_root / "part2" / "compiled.hef"

        hailo_p1 = HailoSession(hef_p1, quantized_inputs=True, quantized_outputs=False)
        hailo_p2 = HailoSession(hef_p2, quantized_inputs=False, quantized_outputs=False)

        # Build Hailo stage1 inputs.
        p1_inputs_hailo: Dict[str, np.ndarray] = {}
        for name in hailo_p1.input_names:
            src = feeds_p1.get(name)
            if src is None:
                # Fallback: single-input models.
                src = next(iter(feeds_p1.values()))

            src_arr = np.asarray(src)
            # For image-like tensors we convert to uint8 and let Hailo quantization handle scaling.
            try:
                img_u8 = _nchw_to_hwc_u8(src_arr)
                tgt_shape = hailo_p1.input_shapes.get(name)
                if tgt_shape is not None:
                    img_u8 = _adapt_tensor(img_u8, tgt_shape)
                p1_inputs_hailo[name] = img_u8
            except Exception:
                # Non-image / already-quantized input
                tgt_shape = hailo_p1.input_shapes.get(name)
                p1_inputs_hailo[name] = _adapt_tensor(src_arr, tgt_shape) if tgt_shape is not None else src_arr

        def run_p1_map() -> Dict[str, np.ndarray]:
            assert hailo_p1 is not None
            return hailo_p1.infer(p1_inputs_hailo)

        # Precompute mapping and a fixed stage2 input tensor for benchmarking stage2 alone.
        p1_map0_h = run_p1_map()
        p2_in_map = _map_inputs_to_outputs(hailo_p2.input_names, list(p1_map0_h.keys()))

        def _build_p2_inputs_from_p1(out1_map: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
            assert hailo_p2 is not None
            d: Dict[str, np.ndarray] = {}
            for inp in hailo_p2.input_names:
                src_name = p2_in_map.get(inp, inp)
                if src_name in out1_map:
                    src = out1_map[src_name]
                elif inp in feeds_full:
                    src = feeds_full[inp]
                else:
                    raise KeyError(
                        f"Cannot build part2 input '{inp}': not in part1 outputs and not in full inputs. "
                        f"part1 outputs={list(out1_map.keys())} full inputs={list(feeds_full.keys())}"
                    )
                tgt_shape = hailo_p2.input_shapes.get(inp)
                if tgt_shape is not None:
                    src = _adapt_tensor(src, tgt_shape)
                d[inp] = src
            return d

        p2_inputs_hailo_0 = _build_p2_inputs_from_p1(p1_map0_h)

        def run_p2_map() -> Dict[str, np.ndarray]:
            assert hailo_p2 is not None
            return hailo_p2.infer(p2_inputs_hailo_0)

        def run_composed_map() -> Dict[str, np.ndarray]:
            out1 = run_p1_map()
            assert hailo_p2 is not None
            return hailo_p2.infer(_build_p2_inputs_from_p1(out1))

        # Timings: skip full (ORT baseline) for Hailo runs.
        print("[full] skipped timing (Hailo backend)")
        full_mean, full_std = None, None
        p1_mean, p1_std, _ = _bench("part1", run_p1_map, args.warmup, args.runs)
        p2_mean, p2_std, _ = _bench("part2", run_p2_map, args.warmup, args.runs)
        comp_mean, comp_std, _ = _bench("composed", run_composed_map, args.warmup, args.runs)

        # Outputs for diff/viz.
        full_out = run_full()
        comp_out = _order_out_map(run_composed_map())

    else:
        # Pure ORT path.
        graph_runner_metrics = None

        def run_p1() -> List[np.ndarray]:
            return sess_p1.run(None, feeds_p1)

        def run_p2() -> List[np.ndarray]:
            return sess_p2.run(None, feeds_p2_0)

        def run_composed() -> List[np.ndarray]:
            out_p1 = sess_p1.run(None, feeds_p1)
            p1_map = {o.name: a for o, a in zip(sess_p1.get_outputs(), out_p1)}
            return sess_p2.run(None, build_feeds_p2(p1_map))

        # Prefer GraphRunner when available in the suite (splitpoint_runners/).
        if _HAS_SPLITPOINT_RUNNERS:
            try:
                print("[graph-runner] ORT benchmarking via splitpoint_runners.GraphRunner")

                # Backends wrap the already-built ORT sessions (no extra engine builds).
                b_full = _SP_OrtSessionBackend("ort_full", sess_full)
                b_p1 = _SP_OrtSessionBackend("ort_part1", sess_p1)
                b_p2 = _SP_OrtSessionBackend("ort_part2", sess_p2)

                # Static input harnesses.
                h_full = _SP_StaticHarness(feeds_full)
                h_p1 = _SP_StaticHarness(feeds_p1)
                h_p2 = _SP_StaticHarness(feeds_p2_0)

                sample_cfg = _SP_SampleCfg(
                    input_kind="static",
                    image_path=(Path(args.image).resolve() if args.image else None),
                    input_scale=args.image_scale,
                )

                runner_single = _SP_GraphRunner()

                plan_full = _SP_GraphPlan(
                    stages=[
                        _SP_Stage(
                            name="full",
                            backend=b_full,
                            run_cfg=_SP_RunCfg(model_path=full_onnx),
                        )
                    ],
                    sample_cfg=sample_cfg,
                    warmup_runs=int(args.warmup),
                    measured_runs=int(args.runs),
                    objective="latency",
                )
                res_full = runner_single.run_graph(plan_full, h_full, out_dir / "graph_full")

                plan_p1 = _SP_GraphPlan(
                    stages=[
                        _SP_Stage(
                            name="part1",
                            backend=b_p1,
                            run_cfg=_SP_RunCfg(model_path=part1_onnx),
                        )
                    ],
                    sample_cfg=sample_cfg,
                    warmup_runs=int(args.warmup),
                    measured_runs=int(args.runs),
                    objective="latency",
                )
                res_p1 = runner_single.run_graph(plan_p1, h_p1, out_dir / "graph_part1")

                plan_p2 = _SP_GraphPlan(
                    stages=[
                        _SP_Stage(
                            name="part2",
                            backend=b_p2,
                            run_cfg=_SP_RunCfg(model_path=part2_onnx),
                        )
                    ],
                    sample_cfg=sample_cfg,
                    warmup_runs=int(args.warmup),
                    measured_runs=int(args.runs),
                    objective="latency",
                )
                res_p2 = runner_single.run_graph(plan_p2, h_p2, out_dir / "graph_part2")

                # Composed (2-stage) run: stage2 inputs are merged from feeds_p2_0 + stage1 outputs.
                merge_transfer = _SP_MergeP2Transfer(feeds_p2_0)
                runner_comp = _SP_GraphRunner(transfer=merge_transfer)
                plan_comp = _SP_GraphPlan(
                    stages=[
                        _SP_Stage(
                            name="part1",
                            backend=b_p1,
                            run_cfg=_SP_RunCfg(model_path=part1_onnx),
                        ),
                        _SP_Stage(
                            name="part2",
                            backend=b_p2,
                            run_cfg=_SP_RunCfg(model_path=part2_onnx),
                        ),
                    ],
                    sample_cfg=sample_cfg,
                    warmup_runs=int(args.warmup),
                    measured_runs=int(args.runs),
                    objective="latency",
                )
                res_comp = runner_comp.run_graph(plan_comp, h_p1, out_dir / "graph_composed")

                def _get_ms(res: object, *path: str) -> Optional[float]:
                    cur: object = getattr(res, "metrics", {})
                    for key in path:
                        if not isinstance(cur, dict) or key not in cur:
                            return None
                        cur = cur[key]
                    try:
                        return float(cur)  # type: ignore[arg-type]
                    except Exception:
                        return None

                full_mean = _get_ms(res_full, "end_to_end_summary_ms", "measured", "mean")
                full_std = _get_ms(res_full, "end_to_end_summary_ms", "measured", "std")
                p1_mean = _get_ms(res_p1, "end_to_end_summary_ms", "measured", "mean")
                p1_std = _get_ms(res_p1, "end_to_end_summary_ms", "measured", "std")
                p2_mean = _get_ms(res_p2, "end_to_end_summary_ms", "measured", "mean")
                p2_std = _get_ms(res_p2, "end_to_end_summary_ms", "measured", "std")
                comp_mean = _get_ms(res_comp, "end_to_end_summary_ms", "measured", "mean")
                comp_std = _get_ms(res_comp, "end_to_end_summary_ms", "measured", "std")

                full_out = _order_out_map(res_full.outputs if isinstance(res_full.outputs, dict) else {})
                comp_out = _order_out_map(res_comp.outputs if isinstance(res_comp.outputs, dict) else {})

                graph_runner_metrics = {
                    "full": res_full.metrics,
                    "part1": res_p1.metrics,
                    "part2": res_p2.metrics,
                    "composed": res_comp.metrics,
                }
            except Exception as e:
                print(
                    f"[graph-runner] WARNING: GraphRunner path failed ({type(e).__name__}: {e}). "
                    "Falling back to legacy benchmark loop."
                )
                full_mean, full_std, _ = _bench("full", run_full, args.warmup, args.runs)
                p1_mean, p1_std, _ = _bench("part1", run_p1, args.warmup, args.runs)
                p2_mean, p2_std, _ = _bench("part2", run_p2, args.warmup, args.runs)
                comp_mean, comp_std, _ = _bench("composed", run_composed, args.warmup, args.runs)

                full_out = run_full()
                comp_out = run_composed()
        else:
            full_mean, full_std, _ = _bench("full", run_full, args.warmup, args.runs)
            p1_mean, p1_std, _ = _bench("part1", run_p1, args.warmup, args.runs)
            p2_mean, p2_std, _ = _bench("part2", run_p2, args.warmup, args.runs)
            comp_mean, comp_std, _ = _bench("composed", run_composed, args.warmup, args.runs)

            full_out = run_full()
            comp_out = run_composed()

    n = min(len(full_out), len(comp_out))
    per_out = []
    max_abs_global = 0.0
    mean_abs_global = 0.0
    if n > 0:
        for i in range(n):
            a = full_out[i]
            b = comp_out[i]
            da = a.astype(np.float32, copy=False) if isinstance(a, np.ndarray) else np.array(a, dtype=np.float32)
            db = b.astype(np.float32, copy=False) if isinstance(b, np.ndarray) else np.array(b, dtype=np.float32)
            diff = np.abs(da - db)
            max_abs = float(np.max(diff)) if diff.size else 0.0
            mean_abs = float(np.mean(diff)) if diff.size else 0.0
            per_out.append({"index": i, "shape": list(getattr(a, "shape", [])), "max_abs": max_abs, "mean_abs": mean_abs})
            max_abs_global = max(max_abs_global, max_abs)
            mean_abs_global += mean_abs
        mean_abs_global = mean_abs_global / float(n)


    elementwise_pass = (max_abs_global <= float(args.eps)) if n > 0 else False

    print("==== Timings ====")
    print(f"full    : {full_mean:.3f} Â± {full_std:.3f}")
    print(f"part1   : {p1_mean:.3f} Â± {p1_std:.3f}")
    print(f"part2   : {p2_mean:.3f} Â± {p2_std:.3f}")
    print(f"composed: {comp_mean:.3f} Â± {comp_std:.3f}")
    print(f"(note) part1+part2 (means) = {(p1_mean + p2_mean):.3f} ms")
    print(f"(note) composed - full       = {(comp_mean - full_mean):.3f} ms")

    # Determine output format for viz/validation
    # NOTE: The legacy runner path primarily works with ordered output *lists*.
    # For YOLOv10 BN6 + proxy validation we also need a name->tensor mapping.
    # In the GraphRunner path we already have dict outputs; otherwise we fall
    # back to synthetic output names.
    out_full: Dict[str, np.ndarray] = {}
    out_comp: Dict[str, np.ndarray] = {}
    # Only try to use GraphRunner dict outputs if GraphRunner actually ran.
    if graph_runner_metrics is not None:
        try:
            if isinstance(res_full.outputs, dict):
                out_full = res_full.outputs
            if isinstance(res_comp.outputs, dict):
                out_comp = res_comp.outputs
        except Exception:
            out_full = {}
            out_comp = {}

    if not out_full:
        out_full = {f"output{i}": a for i, a in enumerate(full_out)}
    if not out_comp:
        out_comp = {f"output{i}": a for i, a in enumerate(comp_out)}

    output_names = list(out_full.keys())
    full_out_list = list(out_full.values())
    comp_out_names = list(out_comp.keys())
    comp_out_list = list(out_comp.values())

    output_format = _detect_output_format(output_names, full_out_list)
    if output_format == "unknown":
        output_format = _detect_output_format(comp_out_names, comp_out_list)

    mode_used, mode_warnings = _select_validation_mode(args.validation_mode, args.provider, output_format)
    warn_list: List[str] = []
    warn_list.extend(mode_warnings)

    proxy_metrics: Optional[Dict[str, Any]] = None
    proxy_pass: Optional[bool] = None

    if mode_used == "proxy_detections":
        dets_full, w1 = _extract_detections_for_proxy(
            output_format=output_format,
            output_names=output_names,
            outputs=full_out_list,
            img_hw=img_hw,
            yolo_conf=float(args.yolo_conf),
            yolo_iou=float(args.yolo_iou),
            yolo_max_det=int(args.yolo_max_det),
            det_conf=float(args.detr_conf),
            det_iou=float(args.detr_iou),
            det_max_det=int(args.detr_max_det),
            labels=labels,
        )
        dets_comp, w2 = _extract_detections_for_proxy(
            output_format=output_format,
            output_names=comp_out_names,
            outputs=comp_out_list,
            img_hw=img_hw,
            yolo_conf=float(args.yolo_conf),
            yolo_iou=float(args.yolo_iou),
            yolo_max_det=int(args.yolo_max_det),
            det_conf=float(args.detr_conf),
            det_iou=float(args.detr_iou),
            det_max_det=int(args.detr_max_det),
            labels=labels,
        )
        warn_list.extend(w1)
        warn_list.extend(w2)

        fatal_proxy_w = [
            w
            for w in (w1 + w2)
            if w.startswith(("proxy_", "bn6_")) and not w.startswith("ambiguous_")
        ]
        if fatal_proxy_w:
            if args.provider == "tensorrt":
                # TensorRT: proxy is the default, but if we can't decode detections, we soft-pass (timing-only).
                warn_list.append("proxy_validation_skipped_fallback=off")
                mode_used = "off"
            else:
                # Non-TRT: fall back to strict elementwise gating
                warn_list.append("proxy_validation_failed_fallback=strict_elementwise")
                mode_used = "strict_elementwise"
        else:
            proxy_metrics, proxy_pass = _proxy_validate_detections(dets_ref=dets_full, dets_out=dets_comp)

    if mode_used == "off":
        final_pass = True
    elif mode_used == "strict_elementwise":
        final_pass = bool(elementwise_pass)
    elif mode_used == "proxy_detections":
        final_pass = bool(proxy_pass) if proxy_pass is not None else False
    else:
        final_pass = bool(elementwise_pass)

    print("==== Output diff ====")
    print(f"Compared outputs: {n}")
    print(f"max_abs : {max_abs_global}")
    print(f"mean_abs: {mean_abs_global}")
    print(f"elementwise PASS({args.eps}): {elementwise_pass}")
    print(f"[validation] mode={args.validation_mode} used={mode_used} output_format={output_format} final_pass={final_pass}")
    if proxy_metrics is not None:
        print("==== Proxy validation ====")
        print(f"n_ref    : {proxy_metrics.get('n_ref')}")
        print(f"n_out    : {proxy_metrics.get('n_out')}")
        print(f"matched  : {proxy_metrics.get('matched')}")
        print(f"match_ratio      : {proxy_metrics.get('match_ratio')}")
        print(f"mean_iou_matched : {proxy_metrics.get('mean_iou_matched')}")
        print(f"proxy PASS: {proxy_pass}")

    if args.provider == "hailo":
        # Hailo path may not have comparable outputs (and might be partial).
        return 0

    report = dict(
        schema_version=2,
        timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
        run_cfg=run_cfg,
        plan=plan,
        timings=dict(
            full=dict(mean_ms=float(full_mean), std_ms=float(full_std)),
            part1=dict(mean_ms=float(p1_mean), std_ms=float(p1_std)),
            part2=dict(mean_ms=float(p2_mean), std_ms=float(p2_std)),
            composed=dict(mean_ms=float(comp_mean), std_ms=float(comp_std)),
        ),
        output_format=output_format,
        validation_mode=str(args.validation_mode),
        validation_mode_used=mode_used,
        output_diff=dict(
            compared_outputs=int(n),
            max_abs=float(max_abs_global),
            mean_abs=float(mean_abs_global),
            eps=float(args.eps),
            passed=bool(final_pass),
            elementwise_pass=bool(elementwise_pass),
        ),
        elementwise={
            "compared_outputs": int(n),
            "max_abs": float(max_abs_global),
            "mean_abs": float(mean_abs_global),
            "eps": float(args.eps),
            "pass": bool(elementwise_pass),
        },
        proxy=({**proxy_metrics, "pass": bool(proxy_pass)} if proxy_metrics is not None else None),
        final_pass=bool(final_pass),
        warnings=warn_list,
    )

    # Write report JSON
    report_path = out_dir / "validation_report.json"
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)
    print(f"Wrote {report_path}")

    # Write plots (png/pdf) best-effort
    _try_write_report_plots(out_dir, report)

    # Visualization (best-effort)
    if str(args.viz).lower() != 'none':
        try:
            viz_mode = str(args.viz).lower()
            if viz_mode == 'yolo':
                viz_info = _maybe_yolo_viz(
                    out_dir=out_dir,
                    img_path=img_path,
                    img_hw=img_hw,
                    labels=labels,
                    output_names=output_names,
                    full_out=full_out_list,
                    comp_out=comp_out_list,
                    conf_thr=float(args.yolo_conf),
                    iou_thr=float(args.yolo_iou),
                    max_det=int(args.yolo_max_det),
                )
                if viz_info.get('enabled', False):
                    report['viz'] = viz_info
            elif viz_mode == 'detr':
                detr_info = _maybe_detr_viz(
                    out_dir=out_dir,
                    img_path=img_path,
                    img_hw=img_hw,
                    labels=labels,
                    output_names=output_names,
                    full_out=full_out_list,
                    comp_out=comp_out_list,
                    conf_thr=float(args.detr_conf),
                    iou_thr=float(args.detr_iou),
                    max_det=int(args.detr_max_det),
                )
                if detr_info.get('enabled', False):
                    report['viz'] = detr_info
            elif viz_mode == 'cls':
                cls_info = _maybe_classification_viz(
                    out_dir=out_dir,
                    img_path=img_path,
                    output_names=output_names,
                    full_out=full_out_list,
                    comp_out=comp_out_list,
                    labels=labels,
                    topk=5,
                )
                if cls_info.get('enabled', False):
                    report['viz'] = cls_info
            else:  # auto
                viz_info = _maybe_yolo_viz(
                    out_dir=out_dir,
                    img_path=img_path,
                    img_hw=img_hw,
                    labels=labels,
                    output_names=output_names,
                    full_out=full_out_list,
                    comp_out=comp_out_list,
                    conf_thr=float(args.yolo_conf),
                    iou_thr=float(args.yolo_iou),
                    max_det=int(args.yolo_max_det),
                )
                if viz_info.get('enabled', False):
                    report['viz'] = viz_info
                else:
                    detr_info = _maybe_detr_viz(
                        out_dir=out_dir,
                        img_path=img_path,
                        img_hw=img_hw,
                        labels=labels,
                        output_names=output_names,
                        full_out=full_out_list,
                        comp_out=comp_out_list,
                        conf_thr=float(args.detr_conf),
                        iou_thr=float(args.detr_iou),
                        max_det=int(args.detr_max_det),
                    )
                    if detr_info.get('enabled', False):
                        report['viz'] = detr_info
                    else:
                        cls_info = _maybe_classification_viz(
                            out_dir=out_dir,
                            img_path=img_path,
                            output_names=output_names,
                            full_out=full_out_list,
                            comp_out=comp_out_list,
                            labels=labels,
                            topk=5,
                        )
                        if cls_info.get('enabled', False):
                            report['viz'] = cls_info
        except Exception as e:
            warn = f"Visualization failed: {e}"
            print(f"[warn] {warn}")
            warn_list.append(warn)
            report['viz'] = {'enabled': False, 'error': str(e)}

    # Rewrite report if viz added/updated
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2)

    if not final_pass:
        print(f"[warn] validation failed (mode={mode_used}, output_format={output_format}) â€” continuing (soft-fail).")

    # Soft-fail policy: validation mismatch is a WARNING, not a hard failure.
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
